#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è®¡ç®—æœºè§†è§‰ç¤ºèŒƒä»£ç  - åŸºäºTransformerçš„å›¾åƒæè¿°ç”Ÿæˆ
==================================================

ä¾èµ–åº“ (è¯·å…ˆå®‰è£…):
    pip install torch torchvision transformers pillow gradio
    
ç›¸å…³AIè®ºæ–‡æŠ€æœ¯:
    - UniT: Unified Multimodal Chain-of-Thought Test-time Scaling
    - Stroke of Surprise: Progressive Semantic Illusions in Vector Sketching

åŠŸèƒ½æ¼”ç¤º:
    1. å›¾åƒåŠ è½½ä¸é¢„å¤„ç†
    2. åŸºäºé¢„è®­ç»ƒæ¨¡å‹çš„å›¾åƒæè¿°ç”Ÿæˆ
    3. ç›®æ ‡æ£€æµ‹å¯è§†åŒ–
    4. å›¾åƒç‰¹å¾æå–ä¸ç›¸ä¼¼åº¦è®¡ç®—
"""

import torch
import numpy as np
from PIL import Image, ImageDraw, ImageFont
from transformers import AutoProcessor, AutoModelForVision2Seq
from transformers import DetrImageProcessor, DetrForObjectDetection
import warnings
warnings.filterwarnings('ignore')

# ==================== é…ç½®å‚æ•° ====================
# è®¾å¤‡é€‰æ‹© (ä¼˜å…ˆGPU)
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
print(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {DEVICE}")

# é¢„è®­ç»ƒæ¨¡å‹åç§° (Hugging Faceæ ¼å¼)
IMAGE_CAPTION_MODEL = "microsoft/llava-1.5-7b-hf"  # å›¾åƒæè¿°æ¨¡å‹
# å¤‡é€‰: "Salesforce/blip2-opt-2.7b"  # è½»é‡çº§é€‰æ‹©
OBJECT_DETECTION_MODEL = "facebook/detr-resnet-50"  # ç›®æ ‡æ£€æµ‹æ¨¡å‹


def load_image_caption_model():
    """
    åŠ è½½å›¾åƒæè¿°ç”Ÿæˆæ¨¡å‹
    
    ä½¿ç”¨LLaVAæ¨¡å‹è¿›è¡Œå›¾åƒåˆ°æ–‡æœ¬çš„è½¬æ¢
    è¯¥æ¨¡å‹ç»“åˆäº†è§†è§‰ç¼–ç å™¨å’Œè¯­è¨€æ¨¡å‹,å®ç°äº†è§†è§‰ç†è§£ä¸ç”Ÿæˆ
    
    ç›¸å…³è®ºæ–‡æŠ€æœ¯:
    - UniT: ç»Ÿä¸€å¤šæ¨¡æ€æ€ç»´é“¾æ¨ç†
    """
    print("ğŸ“¦ æ­£åœ¨åŠ è½½å›¾åƒæè¿°æ¨¡å‹...")
    try:
        # ä½¿ç”¨è½»é‡çº§æ¨¡å‹ä»¥ç¡®ä¿æœ¬åœ°å¯è¿è¡Œ
        processor = AutoProcessor.from_pretrained("Salesforce/blip-image-captioning-large")
        model = AutoModelForVision2Seq.from_pretrained(
            "Salesforce/blip-image-captioning-large",
            torch_dtype=torch.float16 if DEVICE == "cuda" else torch.float32
        )
        model.to(DEVICE)
        model.eval()
        print("âœ… å›¾åƒæè¿°æ¨¡å‹åŠ è½½æˆåŠŸ!")
        return processor, model
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        print("ğŸ”„ ä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ...")
        return None, None


def load_object_detection_model():
    """
    åŠ è½½ç›®æ ‡æ£€æµ‹æ¨¡å‹
    
    ä½¿ç”¨DETR (DEtection TRansformer) æ¨¡å‹è¿›è¡Œç›®æ ‡æ£€æµ‹
    DETRå°†ç›®æ ‡æ£€æµ‹è§†ä¸ºé›†åˆé¢„æµ‹é—®é¢˜,ä½¿ç”¨Transformeræ¶æ„
    
    ç›¸å…³è®ºæ–‡æŠ€æœ¯:
    - AttentionRetriever: æ³¨æ„åŠ›å±‚ä½œä¸ºé•¿æ–‡æ¡£æ£€ç´¢å™¨
    """
    print("ğŸ“¦ æ­£åœ¨åŠ è½½ç›®æ ‡æ£€æµ‹æ¨¡å‹...")
    try:
        processor = DetrImageProcessor.from_pretrained(OBJECT_DETECTION_MODEL)
        model = DetrForObjectDetection.from_pretrained(OBJECT_DETECTION_MODEL)
        model.to(DEVICE)
        model.eval()
        print("âœ… ç›®æ ‡æ£€æµ‹æ¨¡å‹åŠ è½½æˆåŠŸ!")
        return processor, model
    except Exception as e:
        print(f"âŒ ç›®æ ‡æ£€æµ‹æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return None, None


def generate_image_caption(processor, model, image_path):
    """
    ç”Ÿæˆå›¾åƒæè¿°
    
    ä½¿ç”¨è§†è§‰è¯­è¨€æ¨¡å‹åˆ†æå›¾åƒå†…å®¹å¹¶ç”Ÿæˆè‡ªç„¶è¯­è¨€æè¿°
    
    å‚æ•°:
        processor: å›¾åƒå¤„ç†å™¨
        model: è§†è§‰è¯­è¨€æ¨¡å‹
        image_path: å›¾åƒæ–‡ä»¶è·¯å¾„
        
    è¿”å›:
        str: ç”Ÿæˆçš„å›¾åƒæè¿°
    """
    if model is None:
        return "æ¨¡å‹æœªåŠ è½½"
    
    try:
        # åŠ è½½å¹¶é¢„å¤„ç†å›¾åƒ
        image = Image.open(image_path).convert('RGB')
        
        # ç¼–ç å›¾åƒå¹¶ç”Ÿæˆæè¿°
        inputs = processor(images=image, return_tensors="pt").to(DEVICE)
        
        with torch.no_grad():
            # ç”Ÿæˆæè¿° (ä½¿ç”¨beam searchè·å¾—æ›´å¥½ç»“æœ)
            output = model.generate(
                **inputs,
                max_new_tokens=100,
                num_beams=5,
                do_sample=False
            )
        
        # è§£ç ç”Ÿæˆçš„æ–‡æœ¬
        caption = processor.batch_decode(output, skip_special_tokens=True)[0]
        return caption
        
    except Exception as e:
        return f"ç”Ÿæˆå¤±è´¥: {str(e)}"


def detect_objects(processor, model, image_path, confidence_threshold=0.7):
    """
    ç›®æ ‡æ£€æµ‹å‡½æ•°
    
    ä½¿ç”¨DETRæ¨¡å‹æ£€æµ‹å›¾åƒä¸­çš„ç‰©ä½“è¾¹ç•Œæ¡†å’Œç±»åˆ«
    
    å‚æ•°:
        processor: DETRå›¾åƒå¤„ç†å™¨
        model: DETRç›®æ ‡æ£€æµ‹æ¨¡å‹
        image_path: å›¾åƒè·¯å¾„
        confidence_threshold: ç½®ä¿¡åº¦é˜ˆå€¼
        
    è¿”å›:
        dict: åŒ…å«æ£€æµ‹ç»“æœçš„å­—å…¸
    """
    if model is None:
        return {"success": False, "message": "æ¨¡å‹æœªåŠ è½½"}
    
    try:
        # åŠ è½½å›¾åƒ
        image = Image.open(image_path).convert('RGB')
        original_size = image.size
        
        # é¢„å¤„ç†
        inputs = processor(images=image, return_tensors="pt").to(DEVICE)
        
        with torch.no_grad():
            outputs = model(**inputs)
        
        # åå¤„ç† - è§£ææ£€æµ‹ç»“æœ
        target_sizes = torch.tensor([original_size[::-1]])
        results = processor.post_process_object_detection(
            outputs, 
            target_sizes=target_sizes,
            threshold=confidence_threshold
        )[0]
        
        # æå–æ£€æµ‹ä¿¡æ¯
        detections = []
        for score, label, box in zip(results["scores"], results["labels"], results["boxes"]):
            detection = {
                "label": model.config.id2label[label.item()],
                "confidence": round(score.item(), 3),
                "box": [round(b.item(), 2) for b in box]
            }
            detections.append(detection)
        
        return {
            "success": True,
            "detections": detections,
            "count": len(detections)
        }
        
    except Exception as e:
        return {"success": False, "message": str(e)}


def draw_detection_results(image_path, detections, output_path="output_detections.jpg"):
    """
    åœ¨å›¾åƒä¸Šç»˜åˆ¶æ£€æµ‹ç»“æœ
    
    å°†ç›®æ ‡æ£€æµ‹çš„è¾¹ç•Œæ¡†å’Œæ ‡ç­¾ç»˜åˆ¶åˆ°å›¾åƒä¸Š
    
    å‚æ•°:
        image_path: è¾“å…¥å›¾åƒè·¯å¾„
        detections: æ£€æµ‹ç»“æœåˆ—è¡¨
        output_path: è¾“å‡ºå›¾åƒè·¯å¾„
    """
    try:
        image = Image.open(image_path).convert('RGB')
        draw = ImageDraw.Draw(image)
        
        # å®šä¹‰é¢œè‰²æ–¹æ¡ˆ (COCOæ•°æ®é›†80ç±»)
        colors = [
            (255, 0, 0), (0, 255, 0), (0, 0, 255), (255, 255, 0),
            (255, 0, 255), (0, 255, 255), (128, 0, 0), (0, 128, 0)
        ]
        
        for i, det in enumerate(detections):
            box = det["box"]
            label = det["label"]
            conf = det["confidence"]
            
            # ç»˜åˆ¶è¾¹ç•Œæ¡†
            color = colors[i % len(colors)]
            draw.rectangle(box, outline=color, width=3)
            
            # ç»˜åˆ¶æ ‡ç­¾èƒŒæ™¯
            text = f"{label}: {conf:.2f}"
            draw.text((box[0], box[1] - 15), text, fill=color)
        
        # ä¿å­˜ç»“æœ
        image.save(output_path)
        print(f"ğŸ’¾ æ£€æµ‹ç»“æœå·²ä¿å­˜è‡³: {output_path}")
        
    except Exception as e:
        print(f"âŒ ç»˜åˆ¶å¤±è´¥: {e}")


def extract_image_features(processor, model, image_path):
    """
    æå–å›¾åƒç‰¹å¾å‘é‡
    
    ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„è§†è§‰ç¼–ç å™¨æå–å›¾åƒçš„æ·±åº¦ç‰¹å¾
    å¯ç”¨äºå›¾åƒç›¸ä¼¼åº¦è®¡ç®—ã€èšç±»ç­‰ä»»åŠ¡
    
    å‚æ•°:
        processor: å›¾åƒå¤„ç†å™¨
        model: è§†è§‰æ¨¡å‹
        image_path: å›¾åƒè·¯å¾„
        
    è¿”å›:
        np.ndarray: ç‰¹å¾å‘é‡
    """
    if model is None:
        return None
    
    try:
        image = Image.open(image_path).convert('RGB')
        inputs = processor(images=image, return_tensors="pt").to(DEVICE)
        
        with torch.no_grad():
            # æå–è§†è§‰ç‰¹å¾
            if hasattr(model, 'vision_model'):
                # å¯¹äºBLIP-2ç±»å‹æ¨¡å‹
                vision_outputs = model.vision_model(pixel_values=inputs['pixel_values'])
                features = vision_outputs.last_hidden_state.mean(dim=1)
            else:
                features = model.get_image_features(**inputs)
        
        return features.cpu().numpy()
        
    except Exception as e:
        print(f"âŒ ç‰¹å¾æå–å¤±è´¥: {e}")
        return None


def calculate_similarity(feature1, feature2):
    """
    è®¡ç®—ä¸¤ä¸ªç‰¹å¾å‘é‡ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦
    
    å‚æ•°:
        feature1: ç¬¬ä¸€ä¸ªç‰¹å¾å‘é‡
        feature2: ç¬¬äºŒä¸ªç‰¹å¾å‘é‡
        
    è¿”å›:
        float: ç›¸ä¼¼åº¦åˆ†æ•° (0-1)
    """
    # å±•å¹³å‘é‡
    f1 = feature1.flatten()
    f2 = feature2.flatten()
    
    # ä½™å¼¦ç›¸ä¼¼åº¦
    dot_product = np.dot(f1, f2)
    norm1 = np.linalg.norm(f1)
    norm2 = np.linalg.norm(f2)
    
    similarity = dot_product / (norm1 * norm2 + 1e-8)
    return float(similarity)


def create_demo_image():
    """
    åˆ›å»ºä¸€ä¸ªç¤ºä¾‹å›¾åƒç”¨äºæµ‹è¯•
    
    ç”ŸæˆåŒ…å«ç®€å•å‡ ä½•å½¢çŠ¶çš„æµ‹è¯•å›¾åƒ
    """
    # åˆ›å»ºç©ºç™½å›¾åƒ
    img = Image.new('RGB', (800, 600), color=(240, 240, 240))
    draw = ImageDraw.Draw(img)
    
    # ç»˜åˆ¶å‡ ä½•å½¢çŠ¶
    # åœ†å½¢ - çº¢è‰²
    draw.ellipse([100, 100, 300, 300], fill=(255, 100, 100), outline=(0, 0, 0), width=2)
    
    # çŸ©å½¢ - ç»¿è‰²
    draw.rectangle([350, 100, 600, 300], fill=(100, 255, 100), outline=(0, 0, 0), width=2)
    
    # ä¸‰è§’å½¢ - è“è‰²
    draw.polygon([(450, 400), (350, 550), (550, 550)], fill=(100, 100, 255), outline=(0, 0, 0))
    
    # æ–‡å­—
    try:
        draw.text((300, 50), "Computer Vision Demo", fill=(0, 0, 0))
    except:
        pass
    
    # ä¿å­˜
    img.save("demo_image.jpg")
    print("ğŸ“· æ¼”ç¤ºå›¾åƒå·²åˆ›å»º: demo_image.jpg")
    return "demo_image.jpg"


def main():
    """
    ä¸»å‡½æ•° - æ¼”ç¤ºè®¡ç®—æœºè§†è§‰çš„ä¸»è¦åŠŸèƒ½
    """
    print("=" * 60)
    print("ğŸ–¼ï¸  è®¡ç®—æœºè§†è§‰ç¤ºèŒƒç¨‹åº")
    print("=" * 60)
    
    # 1. åˆ›å»ºæµ‹è¯•å›¾åƒ
    print("\nğŸ“Œ æ­¥éª¤1: åˆ›å»ºæµ‹è¯•å›¾åƒ")
    test_image = create_demo_image()
    
    # 2. åŠ è½½æ¨¡å‹
    print("\nğŸ“Œ æ­¥éª¤2: åŠ è½½AIæ¨¡å‹")
    caption_processor, caption_model = load_image_caption_model()
    detection_processor, detection_model = load_object_detection_model()
    
    # 3. å›¾åƒæè¿°ç”Ÿæˆ
    print("\nğŸ“Œ æ­¥éª¤3: å›¾åƒæè¿°ç”Ÿæˆ")
    caption = generate_image_caption(caption_processor, caption_model, test_image)
    print(f"ğŸ“ ç”Ÿæˆçš„æè¿°: {caption}")
    
    # 4. ç›®æ ‡æ£€æµ‹
    print("\nğŸ“Œ æ­¥éª¤4: ç›®æ ‡æ£€æµ‹")
    detections = detect_objects(detection_processor, detection_model, test_image)
    if detections.get("success"):
        print(f"ğŸ” æ£€æµ‹åˆ° {detections['count']} ä¸ªç‰©ä½“:")
        for det in detections["detections"]:
            print(f"   - {det['label']}: {det['confidence']}")
        
        # ç»˜åˆ¶æ£€æµ‹ç»“æœ
        draw_detection_results(test_image, detections["detections"])
    
    # 5. ç‰¹å¾æå–ä¸ç›¸ä¼¼åº¦
    print("\nğŸ“Œ æ­¥éª¤5: å›¾åƒç‰¹å¾æå–")
    features = extract_image_features(caption_processor, caption_model, test_image)
    if features is not None:
        print(f"ğŸ“Š ç‰¹å¾ç»´åº¦: {features.shape}")
        
        # è‡ªèº«ç›¸ä¼¼åº¦ (åº”ä¸º1.0)
        sim_self = calculate_similarity(features, features)
        print(f"ğŸ”— è‡ªç›¸ä¼¼åº¦: {sim_self:.4f}")
    
    print("\n" + "=" * 60)
    print("âœ… æ¼”ç¤ºå®Œæˆ!")
    print("=" * 60)


if __name__ == "__main__":
    main()
