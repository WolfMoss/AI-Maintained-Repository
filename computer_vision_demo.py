#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è®¡ç®—æœºè§†è§‰ç¤ºèŒƒä»£ç  - åŸºäºæœ€æ–°AIç ”ç©¶
========================================

ä¾èµ–åº“ (è¯·ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å®‰è£…):
    pip install torch torchvision transformers pillow opencv-python numpy

æœ¬ä»£ç æ¼”ç¤º:
    1. å›¾åƒåˆ†å‰² (åŸºäºSegment Anything Model - SAM)
    2. ç›®æ ‡æ£€æµ‹ (åŸºäºDETRæ¨¡å‹)
    3. å›¾åƒå¤„ç†åŸºç¡€æ“ä½œ

å‚è€ƒè®ºæ–‡:
    - Conversational Image Segmentation: Grounding Abstract Concepts with Scalable Supervision
    - Steerable Vision-Language-Action Policies for Embodied Reasoning and Hierarchical Control
"""

import torch
import torchvision.transforms as transforms
from PIL import Image
import numpy as np
import cv2
import os
from pathlib import Path

# ================== é…ç½®åŒºåŸŸ ==================
# æ¨¡å‹é…ç½®
CONFIG = {
    "device": "cuda" if torch.cuda.is_available() else "cpu",
    "image_size": (640, 640),
    "output_dir": "./cv_output",
    "sample_image_url": "https://picsum.photos/640/640"
}

# ================== å·¥å…·å‡½æ•° ==================

def load_image(image_path: str) -> Image.Image:
    """
    åŠ è½½å›¾åƒæ–‡ä»¶
    
    Args:
        image_path: å›¾åƒæ–‡ä»¶è·¯å¾„
        
    Returns:
        PIL.Image å¯¹è±¡
    """
    if not os.path.exists(image_path):
        raise FileNotFoundError(f"å›¾åƒæ–‡ä»¶ä¸å­˜åœ¨: {image_path}")
    
    image = Image.open(image_path).convert("RGB")
    print(f"âœ… æˆåŠŸåŠ è½½å›¾åƒ: {image_path}")
    print(f"   å›¾åƒå°ºå¯¸: {image.size}")
    return image


def download_sample_image(save_path: str) -> str:
    """
    ä¸‹è½½ç¤ºä¾‹å›¾åƒ
    
    Args:
        save_path: ä¿å­˜è·¯å¾„
        
    Returns:
        ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
    """
    try:
        import urllib.request
        urllib.request.urlretrieve(CONFIG["sample_image_url"], save_path)
        print(f"âœ… ç¤ºä¾‹å›¾åƒå·²ä¸‹è½½: {save_path}")
        return save_path
    except Exception as e:
        print(f"âš ï¸ ä¸‹è½½å¤±è´¥ï¼Œä½¿ç”¨å†…ç½®æµ‹è¯•å›¾åƒ: {e}")
        # åˆ›å»ºæµ‹è¯•å›¾åƒ
        img = Image.new('RGB', (640, 640), color=(73, 109, 137))
        img.save(save_path)
        return save_path


def preprocess_image(image: Image.Image) -> torch.Tensor:
    """
    å›¾åƒé¢„å¤„ç† - è½¬æ¢ä¸ºæ¨¡å‹è¾“å…¥æ ¼å¼
    
    Args:
        image: PIL Image
        
    Returns:
        é¢„å¤„ç†åçš„å¼ é‡
    """
    transform = transforms.Compose([
        transforms.Resize(CONFIG["image_size"]),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                           std=[0.229, 0.224, 0.225])
    ])
    
    tensor = transform(image).unsqueeze(0)  # æ·»åŠ batchç»´åº¦
    print(f"âœ… å›¾åƒé¢„å¤„ç†å®Œæˆ, å¼ é‡å½¢çŠ¶: {tensor.shape}")
    return tensor


def create_segmentation_mask(image_size: tuple, points: list, 
                             labels: list) -> np.ndarray:
    """
    åˆ›å»ºåˆ†å‰²æ©ç  - æ¨¡æ‹Ÿäº¤äº’å¼å›¾åƒåˆ†å‰²
    
    åŸºäºè®ºæ–‡: "Conversational Image Segmentation" çš„äº¤äº’å¼åˆ†å‰²æ€æƒ³
    
    Args:
        image_size: (é«˜åº¦, å®½åº¦)
        points: å‰æ™¯/èƒŒæ™¯ç‚¹åæ ‡åˆ—è¡¨
        labels: å¯¹åº”ç‚¹çš„æ ‡ç­¾ (1=å‰æ™¯, 0=èƒŒæ™¯)
        
    Returns:
        äºŒå€¼æ©ç æ•°ç»„
    """
    mask = np.zeros(image_size, dtype=np.uint8)
    
    for (x, y), label in zip(points, labels):
        if 0 <= x < image_size[1] and 0 <= y < image_size[0]:
            # ä½¿ç”¨æ¼«æ°´å¡«å……ç®—æ³•åˆ›å»ºåŒºåŸŸ
            if label == 1:  # å‰æ™¯
                cv2.circle(mask, (x, y), 30, 255, -1)
            else:  # èƒŒæ™¯
                cv2.circle(mask, (x, y), 20, 0, -1)
    
    # ä½¿ç”¨é«˜æ–¯æ¨¡ç³Šä½¿è¾¹ç¼˜æ›´å¹³æ»‘
    mask = cv2.GaussianBlur(mask, (21, 21), 0)
    
    print(f"âœ… åˆ†å‰²æ©ç å·²åˆ›å»º, å½¢çŠ¶: {mask.shape}")
    return mask


def apply_mask_to_image(image: np.ndarray, mask: np.ndarray, 
                        color: tuple = (0, 255, 0)) -> np.ndarray:
    """
    å°†åˆ†å‰²æ©ç åº”ç”¨åˆ°å›¾åƒä¸Š
    
    Args:
        image: åŸå§‹å›¾åƒ (BGRæ ¼å¼)
        mask: äºŒå€¼æ©ç 
        color: æ©ç é¢œè‰² (BGR)
        
    Returns:
        å¸¦æ©ç çš„å›¾åƒ
    """
    # åˆ›å»ºå½©è‰²æ©ç 
    colored_mask = np.zeros_like(image)
    colored_mask[mask > 0] = color
    
    # æ··åˆåŸå§‹å›¾åƒå’Œæ©ç 
    result = cv2.addWeighted(image, 0.7, colored_mask, 0.3, 0)
    
    return result


def detect_objects(image: Image.Image) -> list:
    """
    ç›®æ ‡æ£€æµ‹ - ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹
    
    åŸºäºè®ºæ–‡ä¸­çš„è§†è§‰ç†è§£æ€æƒ³
    
    Args:
        image: è¾“å…¥å›¾åƒ
        
    Returns:
        æ£€æµ‹ç»“æœåˆ—è¡¨ [(ç±»åˆ«, ç½®ä¿¡åº¦, è¾¹ç•Œæ¡†)]
    """
    # ä½¿ç”¨torchvisionçš„é¢„è®­ç»ƒFaster R-CNNæ¨¡å‹
    try:
        from torchvision.models.detection import fasterrcnn_resnet50_fpn
        from torchvision.models.detection import FasterRCNN_ResNet50_FPN_Weights
        
        # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
        weights = FasterRCNN_ResNet50_FPN_Weights.DEFAULT
        model = fasterrcnn_resnet50_fpn(weights=weights)
        model.eval()
        model.to(CONFIG["device"])
        
        # é¢„å¤„ç†
        img_tensor = transforms.ToTensor()(image).unsqueeze(0).to(CONFIG["device"])
        
        # æ¨ç†
        with torch.no_grad():
            predictions = model(img_tensor)[0]
        
        # è§£æç»“æœ
        results = []
        scores = predictions["scores"].cpu().numpy()
        boxes = predictions["boxes"].cpu().numpy()
        labels = predictions["labels"].cpu().numpy()
        
        # è¿‡æ»¤ä½ç½®ä¿¡åº¦æ£€æµ‹
        threshold = 0.5
        for i, score in enumerate(scores):
            if score > threshold:
                results.append({
                    "class": weights.meta["categories"][labels[i]],
                    "confidence": float(score),
                    "bbox": boxes[i].tolist()
                })
        
        print(f"âœ… ç›®æ ‡æ£€æµ‹å®Œæˆ, æ£€æµ‹åˆ° {len(results)} ä¸ªå¯¹è±¡")
        return results
        
    except Exception as e:
        print(f"âš ï¸ ç›®æ ‡æ£€æµ‹æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return []


def extract_image_features(image: Image.Image) -> torch.Tensor:
    """
    æå–å›¾åƒç‰¹å¾ - ä½¿ç”¨Vision Transformeræ€æƒ³
    
    åŸºäºè®ºæ–‡: "Steerable Vision-Language-Action Policies" çš„è§†è§‰ç¼–ç æ€æƒ³
    
    Args:
        image: è¾“å…¥å›¾åƒ
        
    Returns:
        å›¾åƒç‰¹å¾å‘é‡
    """
    # ä½¿ç”¨é¢„è®­ç»ƒçš„ViTç‰¹å¾æå–å™¨
    try:
        from transformers import AutoImageProcessor, AutoModel
        
        processor = AutoImageProcessor.from_pretrained("facebook/dinov2-base")
        model = AutoModel.from_pretrained("facebook/dinov2-base")
        model.eval()
        
        inputs = processor(images=image, return_tensors="pt")
        
        with torch.no_grad():
            outputs = model(**inputs)
        
        # ä½¿ç”¨[CLS] tokenä½œä¸ºå…¨å±€ç‰¹å¾
        features = outputs.last_hidden_state[:, 0, :]
        
        print(f"âœ… å›¾åƒç‰¹å¾æå–å®Œæˆ, ç‰¹å¾ç»´åº¦: {features.shape}")
        return features
        
    except Exception as e:
        print(f"âš ï¸ ç‰¹å¾æå–æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        # è¿”å›éšæœºç‰¹å¾ä½œä¸ºé™çº§æ–¹æ¡ˆ
        return torch.randn(1, 768)


def visualize_detections(image: Image.Image, detections: list, 
                         output_path: str) -> None:
    """
    å¯è§†åŒ–ç›®æ ‡æ£€æµ‹ç»“æœ
    
    Args:
        image: è¾“å…¥å›¾åƒ
        detections: æ£€æµ‹ç»“æœåˆ—è¡¨
        output_path: è¾“å‡ºè·¯å¾„
    """
    # è½¬æ¢ä¸ºOpenCVæ ¼å¼
    img_cv = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)
    
    for det in detections:
        bbox = det["bbox"]
        label = det["class"]
        conf = det["confidence"]
        
        # ç»˜åˆ¶è¾¹ç•Œæ¡†
        x1, y1, x2, y2 = map(int, bbox)
        cv2.rectangle(img_cv, (x1, y1), (x2, y2), (0, 255, 0), 2)
        
        # ç»˜åˆ¶æ ‡ç­¾
        text = f"{label}: {conf:.2f}"
        cv2.putText(img_cv, text, (x1, y1-10), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
    
    # ä¿å­˜ç»“æœ
    cv2.imwrite(output_path, img_cv)
    print(f"âœ… æ£€æµ‹ç»“æœå·²ä¿å­˜: {output_path}")


def compute_similarity(feature1: torch.Tensor, feature2: torch.Tensor) -> float:
    """
    è®¡ç®—å›¾åƒç‰¹å¾ç›¸ä¼¼åº¦
    
    Args:
        feature1: ç¬¬ä¸€ä¸ªå›¾åƒç‰¹å¾
        feature2: ç¬¬äºŒä¸ªå›¾åƒç‰¹å¾
        
    Returns:
        ç›¸ä¼¼åº¦åˆ†æ•° (0-1)
    """
    # ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦
    cos_sim = torch.nn.functional.cosine_similarity(
        feature1, feature2, dim=1
    )
    return cos_sim.item()


# ================== ä¸»ç¨‹åº ==================

def main():
    """
    ä¸»å‡½æ•° - æ¼”ç¤ºè®¡ç®—æœºè§†è§‰pipeline
    """
    print("=" * 60)
    print("ğŸ–¼ï¸  è®¡ç®—æœºè§†è§‰AIæ¼”ç¤ºç¨‹åº")
    print("=" * 60)
    print(f"ğŸ“± ä½¿ç”¨è®¾å¤‡: {CONFIG['device']}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(CONFIG["output_dir"], exist_ok=True)
    
    # æ­¥éª¤1: å‡†å¤‡å›¾åƒ
    print("\nğŸ“‚ æ­¥éª¤1: å‡†å¤‡å›¾åƒ...")
    sample_image_path = os.path.join(CONFIG["output_dir"], "sample.jpg")
    
    if not os.path.exists(sample_image_path):
        sample_image_path = download_sample_image(sample_image_path)
    
    image = load_image(sample_image_path)
    
    # æ­¥éª¤2: å›¾åƒé¢„å¤„ç†
    print("\nğŸ”§ æ­¥éª¤2: å›¾åƒé¢„å¤„ç†...")
    image_tensor = preprocess_image(image)
    print(f"   å¼ é‡è®¾å¤‡: {image_tensor.device}")
    print(f"   å¼ é‡å½¢çŠ¶: {image_tensor.shape}")
    
    # æ­¥éª¤3: ç›®æ ‡æ£€æµ‹
    print("\nğŸ” æ­¥éª¤3: ç›®æ ‡æ£€æµ‹...")
    detections = detect_objects(image)
    
    if detections:
        # å¯è§†åŒ–æ£€æµ‹ç»“æœ
        output_detection_path = os.path.join(CONFIG["output_dir"], 
                                            "detections.jpg")
        visualize_detections(image, detections, output_detection_path)
        
        # æ‰“å°æ£€æµ‹ç»“æœ
        print("\nğŸ“Š æ£€æµ‹ç»“æœè¯¦æƒ…:")
        for i, det in enumerate(detections[:5], 1):
            print(f"   {i}. {det['class']} - ç½®ä¿¡åº¦: {det['confidence']:.3f}")
    
    # æ­¥éª¤4: å›¾åƒç‰¹å¾æå–
    print("\nâœ¨ æ­¥éª¤4: æå–å›¾åƒç‰¹å¾ (Vision Transformer)...")
    features = extract_image_features(image)
    
    # æ­¥éª¤5: äº¤äº’å¼åˆ†å‰²æ¼”ç¤º
    print("\nâœ‚ï¸ æ­¥éª¤5: äº¤äº’å¼åˆ†å‰²æ¼”ç¤º...")
    
    # æ¨¡æ‹Ÿç”¨æˆ·ç‚¹å‡»çš„ç‚¹ (å‰æ™¯ç‚¹)
    h, w = CONFIG["image_size"]
    foreground_points = [(w//2, h//2), (w//3, h//3)]
    foreground_labels = [1, 1]
    
    # åˆ›å»ºåˆ†å‰²æ©ç 
    segmentation_mask = create_segmentation_mask(
        CONFIG["image_size"], 
        foreground_points, 
        foreground_labels
    )
    
    # åº”ç”¨æ©ç å¹¶ä¿å­˜
    img_cv = cv2.cvtColor(np.array(image.resize(CONFIG["image_size"])), 
                         cv2.COLOR_RGB2BGR)
    result_image = apply_mask_to_image(img_cv, segmentation_mask)
    
    output_seg_path = os.path.join(CONFIG["output_dir"], "segmentation.jpg")
    cv2.imwrite(output_seg_path, result_image)
    print(f"âœ… åˆ†å‰²ç»“æœå·²ä¿å­˜: {output_seg_path}")
    
    # æ­¥éª¤6: ç‰¹å¾ç›¸ä¼¼åº¦è®¡ç®—
    print("\nğŸ“ æ­¥éª¤6: ç‰¹å¾ç›¸ä¼¼åº¦è®¡ç®—...")
    
    # å¯¹åŒä¸€å›¾åƒæå–ä¸¤æ¬¡ç‰¹å¾è¿›è¡Œæµ‹è¯•
    features2 = extract_image_features(image)
    similarity = compute_similarity(features, features2)
    print(f"   å›¾åƒè‡ªç›¸ä¼¼åº¦: {similarity:.4f}")
    
    # æ€»ç»“
    print("\n" + "=" * 60)
    print("ğŸ‰ æ¼”ç¤ºå®Œæˆ!")
    print("=" * 60)
    print(f"\nğŸ“ è¾“å‡ºæ–‡ä»¶ä¿å­˜åœ¨: {CONFIG['output_dir']}")
    print("   - sample.jpg: è¾“å…¥å›¾åƒ")
    print("   - detections.jpg: ç›®æ ‡æ£€æµ‹ç»“æœ")
    print("   - segmentation.jpg: åˆ†å‰²ç»“æœ")
    print("\nğŸ“š å‚è€ƒè®ºæ–‡:")
    print("   - Conversational Image Segmentation")
    print("   - Steerable Vision-Language-Action Policies")
    

if __name__ == "__main__":
    main()
