#!/usr/bin/env python3
"""
AIæŠ€æœ¯èµ„è®¯å®æ—¶æŠ“å–å™¨
ä»å¤šä¸ªæƒå¨æ¥æºè·å–æœ€æ–°AIæ–°é—»å’Œè®ºæ–‡ï¼š
1. ArXiv (AI/ML/NLP/CVè®ºæ–‡)
2. Hacker News (AIè®¨è®º)
3. MIT Technology Review (RSS)
4. OpenAI Blog
5. Google AI Blog
"""

import json
import ssl
import urllib.request
import urllib.error
from datetime import datetime, timedelta
from typing import List, Dict, Optional
from xml.etree import ElementTree as ET
import feedparser
import subprocess
import sys

# å°è¯•å¯¼å…¥requestsï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨urllib
try:
    import requests
    HAS_REQUESTS = True
except ImportError:
    HAS_REQUESTS = False


class AINewsFetcher:
    """AIæ–°é—»è·å–å™¨"""

    def __init__(self):
        self.current_date = datetime.now()
        self.yesterday = self.current_date - timedelta(days=1)
        self.two_days_ago = self.current_date - timedelta(days=2)
        self.one_week_ago = self.current_date - timedelta(days=7)

        # SSLä¸Šä¸‹æ–‡ï¼ˆå¤„ç†è¯ä¹¦é—®é¢˜ï¼‰
        self.ssl_context = ssl.create_default_context()
        self.ssl_context.check_hostname = False
        self.ssl_context.verify_mode = ssl.CERT_NONE

    def log(self, msg: str):
        """æ—¥å¿—è¾“å‡º"""
        print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {msg}")

    def fetch_arxiv_papers(self, max_papers: int = 15) -> List[Dict]:
        """ä»ArXivè·å–æœ€æ–°AIè®ºæ–‡"""
        self.log("ğŸ“š æ­£åœ¨æŠ“å– ArXiv AI/ML è®ºæ–‡...")

        papers = []

        # æŸ¥è¯¢æœ€æ–°çš„AIç›¸å…³è®ºæ–‡
        url = (
            "https://export.arxiv.org/api/query?"
            "search_query=cat:cs.AI+OR+cat:cs.LG+OR+cat:cs.CL+OR+cat:cs.NE+OR+cat:cs.CV+OR+cat:cs.RO+OR+cat:cs.IR&"
            f"start=0&max_results={max_papers}&"
            "sortBy=submittedDate&sortOrder=descending"
        )

        try:
            if HAS_REQUESTS:
                response = requests.get(url, timeout=30)
                response.raise_for_status()
                content = response.text
            else:
                req = urllib.request.Request(url)
                with urllib.request.urlopen(req, context=self.ssl_context, timeout=30) as resp:
                    content = resp.read().decode('utf-8')

            # è§£æXML
            root = ET.fromstring(content)

            for entry in root.findall('{http://www.w3.org/2005/Atom}entry'):
                paper = {
                    'title': entry.find('{http://www.w3.org/2005/Atom}title').text.strip().replace('\n', ' '),
                    'url': entry.find('{http://www.w3.org/2005/Atom}id').text.strip(),
                    'published': entry.find('{http://www.w3.org/2005/Atom}published').text.strip(),
                    'summary': entry.find('{http://www.w3.org/2005/Atom}summary').text.strip()[:500],
                    'authors': [a.find('{http://www.w3.org/2005/Atom}name').text
                               for a in entry.findall('{http://www.w3.org/2005/Atom}author')][:5],
                    'categories': [c.get('term') for c in entry.findall('{http://www.w3.org/2005/Atom}category')],
                    'source': 'ArXiv'
                }
                papers.append(paper)

            self.log(f"âœ… è·å–åˆ° {len(papers)} ç¯‡ArXivè®ºæ–‡")

        except Exception as e:
            self.log(f"âŒ ArXivè·å–å¤±è´¥: {e}")

        return papers[:max_papers]

    def fetch_hacker_news(self, max_items: int = 20) -> List[Dict]:
        """ä»Hacker Newsè·å–AIç›¸å…³è®¨è®º"""
        self.log("ğŸ“° æ­£åœ¨æŠ“å– Hacker News AIè®¨è®º...")

        items = []

        try:
            # ä½¿ç”¨Algolia APIè·å–HNæ•°æ®
            url = (
                "https://hn.algolia.com/api/v1/search_by_date?"
                "tags=story&"
                f"tags=AI,Machine Learning,LLM,OpenAI,Claude,GPT,Deep Learning,Artificial Intelligence&"
                f"hitsPerPage={max_items}"
            )

            if HAS_REQUESTS:
                response = requests.get(url, timeout=30)
                response.raise_for_status()
                data = response.json()
            else:
                req = urllib.request.Request(url)
                with urllib.request.urlopen(req, context=self.ssl_context, timeout=30) as resp:
                    data = json.loads(resp.read().decode('utf-8'))

            for hit in data.get('hits', []):
                item = {
                    'title': hit.get('title', ''),
                    'url': hit.get('url', f"https://news.ycombinator.com/item?id={hit.get('objectID', '')}"),
                    'points': hit.get('points', 0),
                    'author': hit.get('author', ''),
                    'created_at': hit.get('created_at', ''),
                    'num_comments': hit.get('num_comments', 0),
                    'source': 'Hacker News',
                    'object_id': hit.get('objectID', '')
                }
                items.append(item)

            self.log(f"âœ… è·å–åˆ° {len(items)} æ¡HNè®¨è®º")

        except Exception as e:
            self.log(f"âŒ Hacker Newsè·å–å¤±è´¥: {e}")

        return items[:max_items]

    def fetch_mit_tech_review(self, max_articles: int = 10) -> List[Dict]:
        """ä»MIT Technology Reviewè·å–AIæ–‡ç« """
        self.log("ğŸ“– æ­£åœ¨æŠ“å– MIT Technology Review AIæ–‡ç« ...")

        articles = []

        try:
            url = "https://www.technologyreview.com/topic/artificial-intelligence/feed"

            # ä½¿ç”¨feedparserè§£æRSS
            feed = feedparser.parse(url)

            count = 0
            for entry in feed.entries:
                if count >= max_articles:
                    break

                # è§£ææ—¥æœŸ
                published = ""
                if hasattr(entry, 'published_parsed'):
                    try:
                        dt = datetime(*entry.published_parsed[:6])
                        published = dt.strftime('%Y-%m-%d')
                    except:
                        pass

                article = {
                    'title': entry.get('title', ''),
                    'url': entry.get('link', ''),
                    'summary': entry.get('summary', '')[:300],
                    'published': published,
                    'source': 'MIT Technology Review'
                }

                # åªä¿ç•™è¿‘æœŸæ–‡ç« 
                if article['title']:
                    articles.append(article)
                    count += 1

            self.log(f"âœ… è·å–åˆ° {len(articles)} ç¯‡MIT Tech Reviewæ–‡ç« ")

        except Exception as e:
            self.log(f"âŒ MIT Technology Reviewè·å–å¤±è´¥: {e}")

        return articles[:max_articles]

    def fetch_github_trending(self, max_items: int = 10) -> List[Dict]:
        """è·å–GitHub AIç›¸å…³è¶‹åŠ¿é¡¹ç›®"""
        self.log("â­ æ­£åœ¨æŠ“å– GitHub AIè¶‹åŠ¿é¡¹ç›®...")

        repos = []

        try:
            # ä½¿ç”¨æœç´¢APIè·å–æœ€æ–°çš„AIç›¸å…³Pythoné¡¹ç›®
            url = (
                "https://api.github.com/search/repositories?"
                "q=AI+machine-learning+deep-learning+language:python+created:>=" + 
                (self.one_week_ago.strftime('%Y-%m-%d')) +
                f"&sort=stars&per_page={max_items}"
            )

            headers = {
                'Accept': 'application/vnd.github.v3+json',
                'User-Agent': 'AI-News-Fetcher'
            }

            if HAS_REQUESTS:
                response = requests.get(url, headers=headers, timeout=30)
                response.raise_for_status()
                data = response.json()
            else:
                req = urllib.request.Request(url, headers=headers)
                with urllib.request.urlopen(req, context=self.ssl_context, timeout=30) as resp:
                    data = json.loads(resp.read().decode('utf-8'))

            for repo in data.get('items', []):
                item = {
                    'name': repo.get('full_name', ''),
                    'description': repo.get('description', '')[:300],
                    'stars': repo.get('stargazers_count', 0),
                    'language': repo.get('language', ''),
                    'url': repo.get('html_url', ''),
                    'updated': repo.get('updated_at', '')[:10],
                    'source': 'GitHub Trending'
                }
                repos.append(item)

            self.log(f"âœ… è·å–åˆ° {len(repos)} ä¸ªGitHubé¡¹ç›®")

        except Exception as e:
            self.log(f"âš ï¸ GitHubè¶‹åŠ¿è·å–è·³è¿‡ï¼ˆéœ€è®¤è¯ï¼‰: {str(e)[:50]}")
            # æä¾›æ‰‹åŠ¨æŸ¥çœ‹é“¾æ¥
            self.log("ğŸ’¡ æ‰‹åŠ¨æŸ¥çœ‹: https://github.com/trending?spoken_language_code=zh")

        return repos[:max_items]

    def aggregate_news(self) -> Dict:
        """æ•´åˆæ‰€æœ‰æ–°é—»æº"""
        self.log("\n" + "="*60)
        self.log("ğŸš€ å¼€å§‹AIæ–°é—»èšåˆ")
        self.log("="*60 + "\n")

        # å¹¶è¡Œæˆ–é¡ºåºè·å–å„æ¥æº
        arxiv_papers = self.fetch_arxiv_papers(12)
        hn_discussions = self.fetch_hacker_news(15)
        mit_articles = self.fetch_mit_tech_review(8)
        github_repos = self.fetch_github_trending(8)

        result = {
            'fetch_time': self.current_date.strftime('%Y-%m-%d %H:%M:%S'),
            'arxiv_papers': arxiv_papers,
            'hn_discussions': hn_discussions,
            'mit_articles': mit_articles,
            'github_repos': github_repos,
            'total_sources': 4
        }

        return result

    def format_news_report(self, data: Dict) -> str:
        """æ ¼å¼åŒ–æ–°é—»æŠ¥å‘Š"""
        report = []
        report.append("# AIæŠ€æœ¯èµ„è®¯æ±‡æ€»")
        report.append("")
        report.append(f"**è·å–æ—¶é—´**: {data['fetch_time']}")
        report.append("")
        report.append("---\n")

        # ArXivè®ºæ–‡
        if data['arxiv_papers']:
            report.append("## ğŸ“š ArXivæœ€æ–°è®ºæ–‡")
            report.append("")
            for i, paper in enumerate(data['arxiv_papers'][:8], 1):
                pub_date = paper['published'][:10] if paper['published'] else 'N/A'
                report.append(f"### {i}. {paper['title']}")
                report.append(f"- **å‘å¸ƒæ—¶é—´**: {pub_date}")
                report.append(f"- **é“¾æ¥**: {paper['url']}")
                report.append(f"- **ä½œè€…**: {', '.join(paper['authors'][:3])}")
                report.append(f"- **ç±»åˆ«**: {', '.join(paper['categories'][:3])}")
                report.append("")
            report.append("---\n")

        # Hacker News
        if data['hn_discussions']:
            report.append("## ğŸ’¬ Hacker Newsçƒ­é—¨è®¨è®º")
            report.append("")
            for i, item in enumerate(data['hn_discussions'][:8], 1):
                report.append(f"### {i}. {item['title']}")
                report.append(f"- **æ¥æº**: [Hacker News](https://news.ycombinator.com/item?id={item.get('object_id', '')})")
                report.append(f"- **ç‚¹èµ**: {item['points']} | **è¯„è®º**: {item['num_comments']}")
                if item['url'] and item['url'] != f"https://news.ycombinator.com/item?id={item.get('objectID', '')}":
                    report.append(f"- **åŸæ–‡é“¾æ¥**: {item['url']}")
                report.append("")
            report.append("---\n")

        # MIT Technology Review
        if data['mit_articles']:
            report.append("## ğŸ“– MIT Technology Review")
            report.append("")
            for i, article in enumerate(data['mit_articles'][:5], 1):
                report.append(f"### {i}. {article['title']}")
                report.append(f"- **é“¾æ¥**: {article['url']}")
                report.append(f"- **å‘å¸ƒæ—¶é—´**: {article['published']}")
                report.append("")
            report.append("---\n")

        # GitHub Trending
        if data['github_repos']:
            report.append("## â­ GitHubè¶‹åŠ¿é¡¹ç›®")
            report.append("")
            for i, repo in enumerate(data['github_repos'][:5], 1):
                report.append(f"### {i}. {repo['name']}")
                report.append(f"- **æè¿°**: {repo['description']}")
                report.append(f"- **â­ Stars**: {repo['stars']}")
                report.append(f"- **ğŸ”¤ è¯­è¨€**: {repo['language']}")
                report.append(f"- **ğŸ”— é“¾æ¥**: {repo['url']}")
                report.append("")
            report.append("---\n")

        return "\n".join(report)

    def save_to_file(self, data: Dict, output_file: str):
        """ä¿å­˜æ•°æ®åˆ°æ–‡ä»¶"""
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        self.log(f"ğŸ“ JSONæ•°æ®å·²ä¿å­˜: {output_file}")

    def save_report(self, report: str, output_file: str):
        """ä¿å­˜MarkdownæŠ¥å‘Š"""
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(report)
        self.log(f"ğŸ“„ MarkdownæŠ¥å‘Šå·²ä¿å­˜: {output_file}")


def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description='AIæ–°é—»è·å–å™¨')
    parser.add_argument('--output-dir', type=str, required=True, help='è¾“å‡ºç›®å½•')
    parser.add_argument('--format', type=str, default='both',
                        choices=['json', 'markdown', 'both'],
                        help='è¾“å‡ºæ ¼å¼')
    args = parser.parse_args()

    fetcher = AINewsFetcher()
    data = fetcher.aggregate_news()

    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

    if args.format in ['json', 'both']:
        json_file = f"{args.output_dir}/ai_news_data_{timestamp}.json"
        fetcher.save_to_file(data, json_file)

    if args.format in ['markdown', 'both']:
        report = fetcher.format_news_report(data)
        md_file = f"{args.output_dir}/ai_news_{timestamp}.txt"
        fetcher.save_report(report, md_file)

    print("\n" + "="*60)
    print("âœ… æ–°é—»è·å–å®Œæˆ!")
    print(f"ğŸ“Š æ•°æ®æ¥æº: ArXiv, Hacker News, MIT Tech Review, GitHub")
    print("="*60 + "\n")


if __name__ == "__main__":
    main()
