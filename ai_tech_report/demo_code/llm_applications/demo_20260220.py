#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LLMåº”ç”¨ç¤ºèŒƒä»£ç  - åŸºäºLangChainçš„AIåº”ç”¨ç¤ºä¾‹
ä¾èµ–å®‰è£…: pip install langchain langchain-openai langchain-community python-dotenv

æœ¬ä»£ç å±•ç¤ºå¦‚ä½•æ„å»ºåŸºäºå¤§è¯­è¨€æ¨¡å‹(LLM)çš„åº”ç”¨ç¨‹åº
åŒ…å«ï¼šæ–‡æœ¬æ‘˜è¦ã€é—®ç­”ç³»ç»Ÿã€æ–‡æœ¬åˆ†ç±»ã€å†…å®¹ç”Ÿæˆç­‰å¸¸è§åº”ç”¨åœºæ™¯
"""

# å¯¼å…¥å¿…è¦çš„åº“
import os
from datetime import datetime
from typing import List, Dict, Any

# LangChain æ ¸å¿ƒç»„ä»¶
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_openai import ChatOpenAI
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from langchain.text_splitter import CharacterTextSplitter
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import TextLoader

# ============================================================
# é…ç½®éƒ¨åˆ† - è¯·æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹
# ============================================================

# è®¾ç½®APIå¯†é’¥ (å»ºè®®ä½¿ç”¨ç¯å¢ƒå˜é‡)
# os.environ["OPENAI_API_KEY"] = "your-api-key-here"

# ä½¿ç”¨Ollamaæœ¬åœ°æ¨¡å‹ (å…è´¹ï¼Œæ— éœ€APIå¯†é’¥)
# ç¡®ä¿æœ¬åœ°å·²å®‰è£…Ollamaå¹¶è¿è¡Œ: ollama serve
# æ¨¡å‹åˆ—è¡¨: llama2, mistral, qwen, etc.

class LLMApplicationDemo:
    """
    LLMåº”ç”¨æ¼”ç¤ºç±»
    å±•ç¤ºå¤§è¯­è¨€æ¨¡å‹çš„å¤šç§åº”ç”¨åœºæ™¯
    """
    
    def __init__(self, model_provider: str = "ollama", model_name: str = "llama2"):
        """
        åˆå§‹åŒ–LLMåº”ç”¨
        
        å‚æ•°:
            model_provider: æ¨¡å‹æä¾›å•† ("openai" æˆ– "ollama")
            model_name: æ¨¡å‹åç§°
        """
        self.model_provider = model_provider
        self.model_name = model_name
        self.llm = self._create_llm()
        print(f"âœ“ LLMåº”ç”¨åˆå§‹åŒ–å®Œæˆï¼Œä½¿ç”¨æ¨¡å‹: {model_name}")
        print(f"  æä¾›å•†: {model_provider}")
        
    def _create_llm(self):
        """åˆ›å»ºLLMå®ä¾‹"""
        if self.model_provider == "openai":
            # OpenAI API è°ƒç”¨ (éœ€è¦APIå¯†é’¥)
            return ChatOpenAI(
                model=self.model_name,
                temperature=0.7,  # æ§åˆ¶ç”Ÿæˆéšæœºæ€§
                max_tokens=1000
            )
        elif self.model_provider == "ollama":
            # Ollama æœ¬åœ°æ¨¡å‹ (å…è´¹å¼€æº)
            from langchain_community.chat_models import ChatOllama
            return ChatOllama(
                model=self.model_name,
                temperature=0.7
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹æä¾›å•†: {self.model_provider}")
    
    def text_summarization(self, text: str) -> str:
        """
        æ–‡æœ¬æ‘˜è¦åŠŸèƒ½
        å°†é•¿æ–‡æœ¬å‹ç¼©æˆç®€æ´çš„æ‘˜è¦
        
        å‚æ•°:
            text: å¾…æ‘˜è¦çš„æ–‡æœ¬
            
        è¿”å›:
            ç”Ÿæˆçš„æ‘˜è¦æ–‡æœ¬
        """
        print("\n" + "="*50)
        print("ğŸ“ åº”ç”¨1: æ–‡æœ¬æ‘˜è¦")
        print("="*50)
        
        # ä½¿ç”¨LangChainçš„æ‘˜è¦é“¾
        prompt_template = """è¯·å°†ä»¥ä¸‹æ–‡æœ¬ç®€æ˜æ‰¼è¦åœ°æ‘˜è¦æˆä¸­æ–‡æ‘˜è¦:

{text}

æ‘˜è¦:"""

        prompt = PromptTemplate(
            template=prompt_template,
            input_variables=["text"]
        )
        
        chain = LLMChain(llm=self.llm, prompt=prompt)
        summary = chain.run(text=text)
        
        print(f"åŸæ–‡é•¿åº¦: {len(text)} å­—ç¬¦")
        print(f"æ‘˜è¦: {summary}")
        return summary
    
    def question_answering(self, context: str, question: str) -> str:
        """
        é—®ç­”ç³»ç»Ÿ
        åŸºäºç»™å®šä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ (RAGåŸºæœ¬åŸç†)
        
        å‚æ•°:
            context: èƒŒæ™¯ä¸Šä¸‹æ–‡
            question: é—®é¢˜
            
        è¿”å›:
            é—®é¢˜çš„ç­”æ¡ˆ
        """
        print("\n" + "="*50)
        print("â“ åº”ç”¨2: é—®ç­”ç³»ç»Ÿ")
        print("="*50)
        
        prompt_template = """åŸºäºä»¥ä¸‹èƒŒæ™¯ä¿¡æ¯å›ç­”é—®é¢˜ã€‚å¦‚æœä¿¡æ¯ä¸è¶³ä»¥å›ç­”ï¼Œè¯·è¯´æ˜"ä¿¡æ¯ä¸è¶³"ã€‚

èƒŒæ™¯ä¿¡æ¯:
{context}

é—®é¢˜: {question}

å›ç­”:"""

        prompt = PromptTemplate(
            template=prompt_template,
            input_variables=["context", "question"]
        )
        
        chain = LLMChain(llm=self.llm, prompt=prompt)
        answer = chain.run(context=context, question=question)
        
        print(f"é—®é¢˜: {question}")
        print(f"ç­”æ¡ˆ: {answer}")
        return answer
    
    def text_classification(self, text: str, categories: List[str]) -> Dict[str, float]:
        """
        æ–‡æœ¬åˆ†ç±»
        å°†æ–‡æœ¬åˆ†ç±»åˆ°é¢„å®šä¹‰çš„ç±»åˆ«ä¸­
        
        å‚æ•°:
            text: å¾…åˆ†ç±»æ–‡æœ¬
            categories: ç±»åˆ«åˆ—è¡¨
            
        è¿”å›:
            å„ç±»åˆ«çš„æ¦‚ç‡
        """
        print("\n" + "="*50)
        print("ğŸ·ï¸ åº”ç”¨3: æ–‡æœ¬åˆ†ç±»")
        print("="*50)
        
        categories_str = ", ".join(categories)
        
        prompt_template = """è¯·åˆ†æä»¥ä¸‹æ–‡æœ¬ï¼Œåˆ¤æ–­å®ƒå±äºå“ªä¸ªç±»åˆ«ã€‚

å¯é€‰ç±»åˆ«: {categories}

æ–‡æœ¬: {text}

è¯·ç›´æ¥è¾“å‡ºæœ€å¯èƒ½çš„ç±»åˆ«åç§°ï¼Œä¸è¦å…¶ä»–è§£é‡Šã€‚"""

        prompt = PromptTemplate(
            template=prompt_template,
            input_variables=["categories", "text"]
        )
        
        chain = LLMChain(llm=self.llm, prompt=prompt)
        result = chain.run(categories=categories_str, text=text)
        
        print(f"æ–‡æœ¬: {text[:50]}...")
        print(f"åˆ†ç±»ç»“æœ: {result.strip()}")
        return {"category": result.strip(), "confidence": 1.0}
    
    def content_generation(self, topic: str, content_type: str = "article") -> str:
        """
        å†…å®¹ç”Ÿæˆ
        æ ¹æ®ä¸»é¢˜ç”Ÿæˆå„ç±»å†…å®¹
        
        å‚æ•°:
            topic: ä¸»é¢˜
            content_type: å†…å®¹ç±»å‹ (article, poem, email, etc.)
            
        ç”Ÿæˆçš„å†…å®¹
        """
        print("\n" + "="*50)
        print("âœï¸ åº”ç”¨4: å†…å®¹ç”Ÿæˆ")
        print("="*50)
        
        type_descriptions = {
            "article": "ä¸€ç¯‡ç»“æ„æ¸…æ™°ã€è®ºè¿°æœ‰åŠ›çš„æ–‡ç« ",
            "poem": "ä¸€é¦–ä¸­æ–‡è¯—æ­Œ",
            "email": "ä¸€å°ä¸“ä¸šçš„ç”µå­é‚®ä»¶",
            "summary": "ç®€æ´çš„æ€»ç»“",
            "code": "Pythonä»£ç ç¤ºä¾‹"
        }
        
        description = type_descriptions.get(content_type, "å†…å®¹")
        
        prompt_template = """è¯·æ ¹æ®ä»¥ä¸‹ä¸»é¢˜ç”Ÿæˆ{description}:

ä¸»é¢˜: {topic}

å†…å®¹:"""

        prompt = PromptTemplate(
            template=prompt_template,
            input_variables=["topic", "description"]
        )
        
        chain = LLMChain(llm=self.llm, prompt=prompt)
        content = chain.run(topic=topic, description=description)
        
        print(f"ä¸»é¢˜: {topic}")
        print(f"ç±»å‹: {content_type}")
        print(f"ç”Ÿæˆå†…å®¹:\n{content}")
        return content
    
    def sentiment_analysis(self, text: str) -> str:
        """
        æƒ…æ„Ÿåˆ†æ
        åˆ†ææ–‡æœ¬çš„æƒ…æ„Ÿå€¾å‘
        
        å‚æ•°:
            text: å¾…åˆ†ææ–‡æœ¬
            
        è¿”å›:
            æƒ…æ„Ÿåˆ†æç»“æœ
        """
        print("\n" + "="*50)
        print("ğŸ˜Š åº”ç”¨5: æƒ…æ„Ÿåˆ†æ")
        print("="*50)
        
        prompt_template = """è¯·åˆ†æä»¥ä¸‹æ–‡æœ¬çš„æƒ…æ„Ÿå€¾å‘ï¼Œåªéœ€è¾“å‡º"æ­£é¢"ã€"è´Ÿé¢"æˆ–"ä¸­æ€§"ï¼š

æ–‡æœ¬: {text}

æƒ…æ„Ÿå€¾å‘:"""

        prompt = PromptTemplate(
            template=prompt_template,
            input_variables=["text"]
        )
        
        chain = LLMChain(llm=self.llm, prompt=prompt)
        sentiment = chain.run(text=text)
        
        print(f"æ–‡æœ¬: {text}")
        print(f"æƒ…æ„Ÿå€¾å‘: {sentiment.strip()}")
        return sentiment.strip()
    
    def run_all_demos(self):
        """è¿è¡Œæ‰€æœ‰æ¼”ç¤º"""
        print("\n" + "ğŸš€"*25)
        print("å¼€å§‹LLMåº”ç”¨æ¼”ç¤º")
        print("="*50)
        
        # ç¤ºä¾‹æ–‡æœ¬ - å…³äºLLMçš„æ–°é—»
        sample_text = """
        äººå·¥æ™ºèƒ½æŠ€æœ¯çš„å¿«é€Ÿå‘å±•æ­£åœ¨æ·±åˆ»æ”¹å˜æˆ‘ä»¬çš„ç”Ÿæ´»æ–¹å¼å’Œå·¥ä½œæ¨¡å¼ã€‚
        å¤§è¯­è¨€æ¨¡å‹(LLM)ä½œä¸ºAIé¢†åŸŸçš„é‡è¦çªç ´ï¼Œå·²ç»åœ¨å„ä¸ªè¡Œä¸šå¾—åˆ°å¹¿æ³›åº”ç”¨ã€‚
        ä»æ™ºèƒ½å®¢æœåˆ°å†…å®¹åˆ›ä½œï¼Œä»æ•°æ®åˆ†æåˆ°ä»£ç ç¼–å†™ï¼ŒLLMå±•ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ã€‚
        ç„¶è€Œï¼Œä¼ ç»Ÿçš„æ•°æ®æ³„éœ²é˜²æŠ¤(DLP)ç³»ç»Ÿåœ¨é¢å¯¹LLMæ—¶æ˜¾å¾—åŠ›ä¸ä»å¿ƒã€‚
        è¿™æ˜¯å› ä¸ºLLMå¯ä»¥å¤„ç†æµ·é‡æ•°æ®ï¼Œä¸”å…¶å·¥ä½œæ–¹å¼ä¸ä¼ ç»Ÿè½¯ä»¶æœ‰æœ¬è´¨åŒºåˆ«ã€‚
        ä¼ä¸šéœ€è¦é‡æ–°æ€è€ƒæ•°æ®å®‰å…¨ç­–ç•¥ï¼Œä»¥åº”å¯¹æ–°æ—¶ä»£çš„å®‰å…¨æŒ‘æˆ˜ã€‚
        """
        
        # 1. æ–‡æœ¬æ‘˜è¦
        self.text_summarization(sample_text)
        
        # 2. é—®ç­”ç³»ç»Ÿ
        context = "LLM(å¤§è¯­è¨€æ¨¡å‹)æ˜¯ä¸€ç§ä½¿ç”¨æ·±åº¦å­¦ä¹ æŠ€æœ¯è®­ç»ƒçš„äººå·¥æ™ºèƒ½æ¨¡å‹ï¼Œ
                  èƒ½å¤Ÿç†è§£å’Œç”Ÿæˆäººç±»è¯­è¨€ã€‚å®ƒä»¬é€šè¿‡å¤§è§„æ¨¡æ–‡æœ¬æ•°æ®è¿›è¡Œé¢„è®­ç»ƒï¼Œ
                  ç„¶åå¯ä»¥é€šè¿‡å¾®è°ƒé€‚åº”ç‰¹å®šä»»åŠ¡ã€‚"
        question = "ä»€ä¹ˆæ˜¯LLMï¼Ÿ"
        self.question_answering(context, question)
        
        # 3. æ–‡æœ¬åˆ†ç±»
        news_text = "NASAå®£å¸ƒå¯åŠ¨æ–°çš„è‰ºæœ¯åˆä½œè®¡åˆ’ï¼Œå°†è‰ºæœ¯å®¶å¸¦å…¥å¤ªç©ºæ¢ç´¢é¡¹ç›®"
        categories = ["ç§‘æŠ€", "è‰ºæœ¯", "ä½“è‚²", "è´¢ç»", "å¨±ä¹"]
        self.text_classification(news_text, categories)
        
        # 4. å†…å®¹ç”Ÿæˆ
        self.content_generation("äººå·¥æ™ºèƒ½å¯¹æœªæ¥å·¥ä½œçš„å½±å“", "article")
        
        # 5. æƒ…æ„Ÿåˆ†æ
        self.sentiment_analysis("è¿™ä¸ªäº§å“çœŸæ˜¯å¤ªæ£’äº†ï¼Œæˆ‘éå¸¸å–œæ¬¢ï¼")
        
        print("\n" + "âœ…"*25)
        print("æ‰€æœ‰æ¼”ç¤ºå®Œæˆ!")
        print("="*50)


def main():
    """
    ä¸»å‡½æ•° - æ¼”ç¤ºLLMçš„å¤šç§åº”ç”¨
    """
    print("="*60)
    print("  ğŸ§  LLMåº”ç”¨ç¤ºèŒƒç¨‹åº")
    print("  åŸºäºLangChainæ„å»ºçš„å¤§è¯­è¨€æ¨¡å‹åº”ç”¨")
    print("="*60)
    
    # åˆ›å»ºLLMåº”ç”¨å®ä¾‹
    # å¯é€‰æä¾›å•†: "openai" (éœ€è¦APIå¯†é’¥) æˆ– "ollama" (æœ¬åœ°å…è´¹)
    demo = LLMApplicationDemo(
        model_provider="ollama",  # ä½¿ç”¨æœ¬åœ°Ollamaæ¨¡å‹
        model_name="llama2"        # å¯é€‰: mistral, qwen, etc.
    )
    
    # è¿è¡Œæ‰€æœ‰æ¼”ç¤º
    demo.run_all_demos()


if __name__ == "__main__":
    # è¿è¡Œä¸»ç¨‹åº
    main()
```

**ä¾èµ–å®‰è£…å‘½ä»¤:**
```bash
pip install langchain langchain-openai langchain-community langchain-ollama python-dotenv
```

**ä½¿ç”¨è¯´æ˜:**
1. æ¨èä½¿ç”¨Ollamaæœ¬åœ°æ¨¡å‹(å…è´¹): å…ˆå®‰è£…Ollamaï¼Œç„¶åè¿è¡Œ`ollama pull llama2`
2. ä¹Ÿå¯ä½¿ç”¨OpenAI API: è®¾ç½®`OPENAI_API_KEY`ç¯å¢ƒå˜é‡å¹¶æ”¹ä¸º`model_provider="openai"`

[2mâ±ï¸  Step 1 completed in 62.99s (total: 62.99s)[0m

[1m[96mSession Statistics:[0m
[2mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€[0m
  Session Duration: 00:01:03
  Total Messages: 3
    - User Messages: [92m1[0m
    - Assistant Replies: [94m1[0m
    - Tool Calls: [93m0[0m
  Available Tools: 8
  API Tokens Used: [95m6,413[0m
[2mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€[0m

