#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AIé•¿æ–‡æœ¬å¤„ç†ä¸é—®ç­”ç³»ç»Ÿ - åŸºäºæœ€æ–°LLMæŠ€æœ¯
============================================
ä¾èµ–åº“:
    pip install transformers torch langchain langchain-community
    pip install sentence-transformers faiss-cpu accelerate

åŠŸèƒ½è¯´æ˜:
    1. é•¿æ–‡æœ¬æ™ºèƒ½åˆ†å—å¤„ç†
    2. åŸºäºè¯­ä¹‰æ£€ç´¢çš„é—®ç­”ç³»ç»Ÿ
    3. æ–‡æœ¬åµŒå…¥å‘é‡ç”Ÿæˆ
    4. ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„ç­”æ¡ˆç”Ÿæˆ

ä½œè€…: AIæ•™è‚²ä¸“å®¶
åŸºäºAIèµ„è®¯: Long Context, Less Focus: A Scaling Gap in LLMs Revealed through Privacy and Personalization
"""

import os
import json
import hashlib
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass, field
from collections import defaultdict

# ==================== ä¾èµ–åº“å¯¼å…¥ ====================
try:
    import torch
    from transformers import (
        AutoTokenizer, 
        AutoModelForCausalLM,
        pipeline,
        BitsAndBytesConfig
    )
    from sentence_transformers import SentenceTransformer
    import numpy as np
    print(f"âœ… PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"âœ… GPUå¯ç”¨: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"âœ… GPUè®¾å¤‡: {torch.cuda.get_device_name(0)}")
except ImportError as e:
    print(f"âŒ ä¾èµ–åº“å¯¼å…¥å¤±è´¥: {e}")
    print("è¯·å®‰è£…: pip install transformers torch sentence-transformers numpy")
    exit(1)


# ==================== é…ç½®ç±» ====================
@dataclass
class LLMConfig:
    """LLMæ¨¡å‹é…ç½®ç±»"""
    # åµŒå…¥æ¨¡å‹é…ç½®
    embedding_model: str = "BAAI/bge-large-zh-v1.5"  # ä¸­æ–‡ä¼˜è´¨åµŒå…¥æ¨¡å‹
    embedding_device: str = "cuda" if torch.cuda.is_available() else "cpu"
    
    # ç”Ÿæˆæ¨¡å‹é…ç½®
    generation_model: str = "Qwen/Qwen2.5-0.5B-Instruct"  # è½»é‡çº§å¯è¿è¡Œæ¨¡å‹
    max_length: int = 512              # æœ€å¤§ç”Ÿæˆé•¿åº¦
    temperature: float = 0.7            # é‡‡æ ·æ¸©åº¦
    top_p: float = 0.9                 # æ ¸é‡‡æ ·æ¦‚ç‡
    context_window: int = 2048          # ä¸Šä¸‹æ–‡çª—å£å¤§å°
    
    # æ–‡æœ¬åˆ†å—é…ç½®
    chunk_size: int = 512              # å—å¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰
    chunk_overlap: int = 50             # å—é‡å å¤§å°
    
    # æ£€ç´¢é…ç½®
    top_k: int = 3                      # æ£€ç´¢Top-Kç»“æœ


@dataclass
class Document:
    """æ–‡æ¡£æ•°æ®ç±»"""
    content: str
    metadata: Dict = field(default_factory=dict)
    
    @property
    def doc_id(self) -> str:
        """ç”Ÿæˆæ–‡æ¡£å”¯ä¸€ID"""
        return hashlib.md5(self.content.encode()).hexdigest()[:8]


# ==================== æ–‡æœ¬å¤„ç†æ¨¡å— ====================
class TextChunker:
    """
    æ™ºèƒ½æ–‡æœ¬åˆ†å—å™¨
    åŠŸèƒ½: å°†é•¿æ–‡æœ¬åˆ†å‰²æˆé€‚åˆLLMå¤„ç†çš„è¾ƒå°å—
    """
    
    def __init__(self, config: LLMConfig):
        self.config = config
    
    def chunk_text(self, text: str, chunk_size: Optional[int] = None) -> List[str]:
        """
        æ–‡æœ¬åˆ†å—ä¸»æ–¹æ³•
        
        Args:
            text: è¾“å…¥é•¿æ–‡æœ¬
            chunk_size: å—å¤§å°ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®ï¼‰
        
        Returns:
            æ–‡æœ¬å—åˆ—è¡¨
        """
        chunk_size = chunk_size or self.config.chunk_size
        overlap = self.config.chunk_overlap
        
        # ç®€å•åˆ†å—ï¼šæŒ‰å›ºå®šé•¿åº¦åˆ†å‰²
        chunks = []
        start = 0
        text_length = len(text)
        
        while start < text_length:
            end = start + chunk_size
            chunk = text[start:end]
            chunks.append(chunk)
            start = end - overlap
        
        print(f"ğŸ“ æ–‡æœ¬åˆ†å—å®Œæˆ: {len(chunks)} ä¸ªå—")
        return chunks
    
    def chunk_by_sentences(self, text: str) -> List[str]:
        """
        æŒ‰å¥å­åˆ†å—ï¼ˆæ›´æ™ºèƒ½çš„åˆ†å—æ–¹å¼ï¼‰
        ä¿æŒå¥å­å®Œæ•´æ€§
        """
        # ç®€å•å¥å·åˆ†å‰²ï¼ˆå®é™…åº”ç”¨ä¸­å¯ä½¿ç”¨æ›´å¤æ‚çš„NLPå·¥å…·ï¼‰
        sentences = text.replace('ã€‚', 'ã€‚|').replace('ï¼', 'ï¼|').replace('ï¼Ÿ', 'ï¼Ÿ|')
        sentences = [s.strip() for s in sentences.split('|') if s.strip()]
        
        chunks = []
        current_chunk = ""
        
        for sentence in sentences:
            if len(current_chunk) + len(sentence) <= self.config.chunk_size:
                current_chunk += sentence
            else:
                if current_chunk:
                    chunks.append(current_chunk)
                current_chunk = sentence
        
        if current_chunk:
            chunks.append(current_chunk)
        
        return chunks


# ==================== åµŒå…¥å‘é‡æ¨¡å— ====================
class EmbeddingGenerator:
    """
    æ–‡æœ¬åµŒå…¥å‘é‡ç”Ÿæˆå™¨
    åŠŸèƒ½: å°†æ–‡æœ¬è½¬æ¢ä¸ºé«˜ç»´å‘é‡ï¼Œç”¨äºè¯­ä¹‰æ£€ç´¢
    """
    
    def __init__(self, config: LLMConfig):
        self.config = config
        self.device = config.embedding_device
        print(f"ğŸ”„ æ­£åœ¨åŠ è½½åµŒå…¥æ¨¡å‹: {config.embedding_model}")
        
        # åŠ è½½å¥å­åµŒå…¥æ¨¡å‹
        self.model = SentenceTransformer(config.embedding_model, device=self.device)
        print(f"âœ… åµŒå…¥æ¨¡å‹åŠ è½½å®Œæˆ (è®¾å¤‡: {self.device})")
    
    def encode(self, texts: List[str], batch_size: int = 32) -> np.ndarray:
        """
        æ‰¹é‡ç”Ÿæˆæ–‡æœ¬åµŒå…¥å‘é‡
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            batch_size: æ‰¹å¤„ç†å¤§å°
        
        Returns:
            åµŒå…¥å‘é‡çŸ©é˜µ (N, D)
        """
        # å½’ä¸€åŒ–åµŒå…¥å‘é‡
        embeddings = self.model.encode(
            texts,
            batch_size=batch_size,
            show_progress_bar=True,
            normalize_embeddings=True
        )
        return embeddings
    
    def compute_similarity(self, vec1: np.ndarray, vec2: np.ndarray) -> float:
        """
        è®¡ç®—ä¸¤ä¸ªå‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦
        """
        return np.dot(vec1, vec2)


# ==================== å‘é‡æ£€ç´¢æ¨¡å— ====================
class VectorRetriever:
    """
    å‘é‡è¯­ä¹‰æ£€ç´¢å™¨
    åŠŸèƒ½: åŸºäºåµŒå…¥å‘é‡çš„ç›¸ä¼¼åº¦æ£€ç´¢
    """
    
    def __init__(self, embedding_generator: EmbeddingGenerator):
        self.embedding_gen = embedding_generator
        self.documents: List[Document] = []
        self.chunks: List[str] = []
        self.chunk_embeddings: Optional[np.ndarray] = None
    
    def add_documents(self, documents: List[Document]) -> None:
        """
        æ·»åŠ æ–‡æ¡£åˆ°æ£€ç´¢ç³»ç»Ÿ
        
        Args:
            documents: Documentå¯¹è±¡åˆ—è¡¨
        """
        chunker = TextChunker(LLMConfig())
        
        all_chunks = []
        for doc in documents:
            # æŒ‰å¥å­æ™ºèƒ½åˆ†å—
            chunks = chunker.chunk_by_sentences(doc.content)
            all_chunks.extend(chunks)
        
        self.chunks = all_chunks
        self.documents = documents
        
        # æ‰¹é‡ç”ŸæˆåµŒå…¥å‘é‡
        print(f"ğŸ”„ æ­£åœ¨ç”Ÿæˆ {len(all_chunks)} ä¸ªæ–‡æœ¬å—çš„åµŒå…¥å‘é‡...")
        self.chunk_embeddings = self.embedding_gen.encode(all_chunks)
        print(f"âœ… åµŒå…¥å‘é‡ç”Ÿæˆå®Œæˆï¼Œç»´åº¦: {self.chunk_embeddings.shape}")
    
    def retrieve(self, query: str, top_k: int = 3) -> List[Tuple[str, float]]:
        """
        æ£€ç´¢ä¸æŸ¥è¯¢æœ€ç›¸å…³çš„æ–‡æœ¬å—
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›Top-Kç»“æœ
        
        Returns:
            (æ–‡æœ¬å—, ç›¸ä¼¼åº¦åˆ†æ•°)åˆ—è¡¨
        """
        # ç”ŸæˆæŸ¥è¯¢åµŒå…¥
        query_embedding = self.embedding_gen.encode([query])
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        similarities = np.dot(self.chunk_embeddings, query_embedding.T).flatten()
        
        # è·å–Top-Kç´¢å¼•
        top_indices = np.argsort(similarities)[::-1][:top_k]
        
        results = []
        for idx in top_indices:
            results.append((self.chunks[idx], similarities[idx]))
        
        return results


# ==================== LLMç”Ÿæˆæ¨¡å— ====================
class LLMGenerator:
    """
    LLMç­”æ¡ˆç”Ÿæˆå™¨
    åŠŸèƒ½: åŸºäºæ£€ç´¢ä¸Šä¸‹æ–‡ç”Ÿæˆè‡ªç„¶è¯­è¨€ç­”æ¡ˆ
    """
    
    def __init__(self, config: LLMConfig):
        self.config = config
        print(f"ğŸ”„ æ­£åœ¨åŠ è½½ç”Ÿæˆæ¨¡å‹: {config.generation_model}")
        
        # åŠ è½½åˆ†è¯å™¨
        self.tokenizer = AutoTokenizer.from_pretrained(
            config.generation_model, 
            trust_remote_code=True
        )
        
        # é‡åŒ–é…ç½®ï¼ˆå‡å°‘æ˜¾å­˜å ç”¨ï¼‰
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.float16
        )
        
        # åŠ è½½æ¨¡å‹ï¼ˆä½¿ç”¨é‡åŒ–å‡å°‘æ˜¾å­˜ï¼‰
        self.model = AutoModelForCausalLM.from_pretrained(
            config.generation_model,
            quantization_config=quantization_config,
            device_map="auto",
            trust_remote_code=True
        )
        
        # åˆ›å»ºç”Ÿæˆç®¡é“
        self.generator = pipeline(
            "text-generation",
            model=self.model,
            tokenizer=self.tokenizer,
            max_length=config.max_length,
            temperature=config.temperature,
            top_p=config.top_p,
            do_sample=True
        )
        print(f"âœ… ç”Ÿæˆæ¨¡å‹åŠ è½½å®Œæˆ")
    
    def generate_answer(
        self, 
        query: str, 
        context_chunks: List[Tuple[str, float]]
    ) -> str:
        """
        åŸºäºä¸Šä¸‹æ–‡ç”Ÿæˆç­”æ¡ˆ
        
        Args:
            query: ç”¨æˆ·é—®é¢˜
            context_chunks: æ£€ç´¢åˆ°çš„ç›¸å…³ä¸Šä¸‹æ–‡
        
        Returns:
            ç”Ÿæˆçš„ç­”æ¡ˆ
        """
        # æ„å»ºæç¤ºè¯
        context = "\n\n".join([chunk[0] for chunk in context_chunks])
        
        prompt = f"""è¯·æ ¹æ®ä»¥ä¸‹å‚è€ƒèµ„æ–™å›ç­”é—®é¢˜ã€‚å¦‚æœèµ„æ–™ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯´æ˜"èµ„æ–™ä¸è¶³"ã€‚

å‚è€ƒèµ„æ–™:
{context}

é—®é¢˜: {query}

å›ç­”:"""
        
        # ç”Ÿæˆç­”æ¡ˆ
        output = self.generator(prompt, max_new_tokens=256)[0]
        answer = output['generated_text'].replace(prompt, "").strip()
        
        return answer


# ==================== ä¸»ç³»ç»Ÿç±» ====================
class LongContextQA:
    """
    é•¿æ–‡æœ¬é—®ç­”ç³»ç»Ÿä¸»ç±»
    æ•´åˆæ‰€æœ‰æ¨¡å—ï¼Œæä¾›ç»Ÿä¸€çš„é—®ç­”æ¥å£
    """
    
    def __init__(self, config: Optional[LLMConfig] = None):
        self.config = config or LLMConfig()
        
        print("=" * 50)
        print("ğŸš€ åˆå§‹åŒ–é•¿æ–‡æœ¬é—®ç­”ç³»ç»Ÿ")
        print("=" * 50)
        
        # åˆå§‹åŒ–å„æ¨¡å—
        self.embedding_gen = EmbeddingGenerator(self.config)
        self.retriever = VectorRetriever(self.embedding_gen)
        self.llm_gen = None  # å»¶è¿ŸåŠ è½½
    
    def load_documents(self, documents: List[Document]) -> None:
        """
        åŠ è½½æ–‡æ¡£åˆ°ç³»ç»Ÿ
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
        """
        print(f"ğŸ“‚ åŠ è½½ {len(documents)} ä¸ªæ–‡æ¡£...")
        self.retriever.add_documents(documents)
    
    def initialize_llm(self) -> None:
        """åˆå§‹åŒ–LLMç”Ÿæˆå™¨ï¼ˆæŒ‰éœ€åŠ è½½ï¼‰"""
        if self.llm_gen is None:
            self.llm_gen = LLMGenerator(self.config)
    
    def query(
        self, 
        question: str, 
        use_llm: bool = True,
        verbose: bool = False
    ) -> Dict:
        """
        æŸ¥è¯¢æ¥å£
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            use_llm: æ˜¯å¦ä½¿ç”¨LLMç”Ÿæˆç­”æ¡ˆ
            verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
        
        Returns:
            åŒ…å«ç­”æ¡ˆå’Œç›¸å…³ä¸Šä¸‹æ–‡çš„å­—å…¸
        """
        # 1. æ£€ç´¢ç›¸å…³ä¸Šä¸‹æ–‡
        context_chunks = self.retriever.retrieve(question, top_k=self.config.top_k)
        
        if verbose:
            print("\nğŸ“‹ æ£€ç´¢åˆ°çš„ç›¸å…³ä¸Šä¸‹æ–‡:")
            for i, (chunk, score) in enumerate(context_chunks, 1):
                print(f"  [{i}] ç›¸ä¼¼åº¦: {score:.4f}")
                print(f"      å†…å®¹: {chunk[:100]}...")
        
        # 2. ç”Ÿæˆç­”æ¡ˆ
        if use_llm:
            self.initialize_llm()
            answer = self.llm_gen.generate_answer(question, context_chunks)
        else:
            # ç®€å•è¿”å›æœ€ç›¸å…³çš„æ–‡æœ¬å—
            answer = context_chunks[0][0] if context_chunks else "æœªæ‰¾åˆ°ç›¸å…³å†…å®¹"
        
        return {
            "question": question,
            "answer": answer,
            "sources": context_chunks
        }


# ==================== ç¤ºä¾‹è¿è¡Œ ====================
def main():
    """ä¸»å‡½æ•°ï¼šæ¼”ç¤ºç³»ç»ŸåŠŸèƒ½"""
    
    print("\n" + "=" * 60)
    print("ğŸ“š AIé•¿æ–‡æœ¬é—®ç­”ç³»ç»Ÿ - ç¤ºä¾‹æ¼”ç¤º")
    print("=" * 60 + "\n")
    
    # ç¤ºä¾‹æ–‡æ¡£ï¼ˆæ¨¡æ‹Ÿé•¿æ–‡æœ¬åœºæ™¯ï¼‰
    sample_docs = [
        Document(
            content="""
            äººå·¥æ™ºèƒ½ï¼ˆArtificial Intelligenceï¼ŒAIï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œ
            å®ƒè¯•å›¾ç†è§£æ™ºèƒ½çš„æœ¬è´¨ï¼Œå¹¶ç”Ÿäº§å‡ºä¸€ç§æ–°çš„èƒ½ä»¥äººç±»æ™ºèƒ½ç›¸ä¼¼çš„æ–¹å¼åšå‡ºååº”çš„æ™ºèƒ½æœºå™¨ã€‚
            è¯¥é¢†åŸŸçš„ç ”ç©¶åŒ…æ‹¬æœºå™¨äººã€è¯­è¨€è¯†åˆ«ã€å›¾åƒè¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†å’Œä¸“å®¶ç³»ç»Ÿç­‰ã€‚
            
            æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„æ ¸å¿ƒï¼Œæ˜¯ä½¿è®¡ç®—æœºå…·æœ‰æ™ºèƒ½çš„æ ¹æœ¬é€”å¾„ã€‚
            å®ƒæ˜¯ä¸€é—¨å¤šé¢†åŸŸäº¤å‰å­¦ç§‘ï¼Œæ¶‰åŠæ¦‚ç‡è®ºã€ç»Ÿè®¡å­¦ã€é€¼è¿‘è®ºã€å‡¸åˆ†æã€ç®—æ³•å¤æ‚åº¦ç†è®ºç­‰å¤šé—¨å­¦ç§‘ã€‚
            æœºå™¨å­¦ä¹ ä¸“é—¨ç ”ç©¶è®¡ç®—æœºæ€æ ·æ¨¡æ‹Ÿæˆ–å®ç°äººç±»çš„å­¦ä¹ è¡Œä¸ºï¼Œä»¥è·å–æ–°çš„çŸ¥è¯†æˆ–æŠ€èƒ½ï¼Œ
            é‡æ–°ç»„ç»‡å·²æœ‰çš„çŸ¥è¯†ç»“æ„ä½¿ä¹‹ä¸æ–­æ”¹å–„è‡ªèº«çš„æ€§èƒ½ã€‚
            
            æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„åˆ†æ”¯ï¼Œæ˜¯ä¸€ç§ä»¥äººå·¥ç¥ç»ç½‘ç»œä¸ºæ¶æ„ï¼Œ
            å¯¹æ•°æ®è¿›è¡Œè¡¨å¾å­¦ä¹ çš„ç®—æ³•ã€‚æ·±åº¦å­¦ä¹ åœ¨è®¡ç®—æœºè§†è§‰ã€è¯­éŸ³è¯†åˆ«ã€
            è‡ªç„¶è¯­è¨€å¤„ç†ç­‰é¢†åŸŸå–å¾—äº†çªç ´æ€§è¿›å±•ã€‚
            """,
            metadata={"source": "AIåŸºç¡€ä»‹ç»", "category": "æŠ€æœ¯ç§‘æ™®"}
        ),
        Document(
            content="""
            å¤§è¯­è¨€æ¨¡å‹ï¼ˆLarge Language Modelï¼ŒLLMï¼‰æ˜¯ä¸€ç§äººå·¥æ™ºèƒ½æ¨¡å‹ï¼Œ
            æ—¨åœ¨ç†è§£å’Œç”Ÿæˆäººç±»è¯­è¨€ã€‚å®ƒä»¬åœ¨å¤§é‡çš„æ–‡æœ¬æ•°æ®ä¸Šè®­ç»ƒï¼Œ
            å¯ä»¥æ‰§è¡Œå„ç§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ï¼Œå¦‚é—®ç­”ã€ç¿»è¯‘ã€æ‘˜è¦ã€å†™ä½œç­‰ã€‚
            
            æœ€æ–°çš„ç ”ç©¶è¡¨æ˜ï¼Œé•¿ä¸Šä¸‹æ–‡èƒ½åŠ›æ˜¯LLMçš„é‡è¦ç‰¹å¾ä¹‹ä¸€ã€‚
            é€šè¿‡æ‰©å±•ä¸Šä¸‹æ–‡çª—å£ï¼Œæ¨¡å‹å¯ä»¥å¤„ç†æ›´é•¿çš„æ–‡æ¡£ï¼Œè¿›è¡Œæ›´æ·±å…¥çš„æ¨ç†ã€‚
            ç„¶è€Œï¼ŒLong Context, Less Focusè®ºæ–‡æŒ‡å‡ºï¼Œé•¿ä¸Šä¸‹æ–‡å¯èƒ½å¸¦æ¥æŒ‘æˆ˜ï¼š
            æ¨¡å‹åœ¨å¤„ç†è¿‡é•¿ä¸Šä¸‹æ–‡æ—¶å¯èƒ½"åˆ†å¿ƒ"ï¼Œå½±å“å¯¹å…³é”®ä¿¡æ¯çš„æ•æ‰ã€‚
            
            ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆIn-Context Learningï¼‰æ˜¯LLMçš„é‡è¦èƒ½åŠ›ï¼Œ
            å…è®¸æ¨¡å‹åœ¨ä¸è¿›è¡Œé¢å¤–è®­ç»ƒçš„æƒ…å†µä¸‹å­¦ä¹ æ–°ä»»åŠ¡ã€‚
            è¿™ç§èƒ½åŠ›éšç€æ¨¡å‹è§„æ¨¡çš„å¢å¤§è€Œå¢å¼ºã€‚
            """,
            metadata={"source": "LLMæŠ€æœ¯ä»‹ç»", "category": "æ·±åº¦å­¦ä¹ "}
        ),
        Document(
            content="""
            Transformeræ¶æ„æ˜¯å½“å‰å¤§è¯­è¨€æ¨¡å‹çš„ä¸»æµæ¶æ„ã€‚
            å®ƒç”±Vaswaniç­‰äººäº2017å¹´æå‡ºï¼Œå®Œå…¨åŸºäºæ³¨æ„åŠ›æœºåˆ¶ã€‚
            Transformeræ¶æ„æ‘’å¼ƒäº†ä¼ ç»Ÿçš„å¾ªç¯å’Œå·ç§¯ç»“æ„ï¼Œ
            é€šè¿‡è‡ªæ³¨æ„åŠ›æœºåˆ¶å®ç°å¹¶è¡Œè®¡ç®—ï¼Œå¤§å¤§æé«˜äº†è®­ç»ƒæ•ˆç‡ã€‚
            
            è‡ªæ³¨æ„åŠ›ï¼ˆSelf-Attentionï¼‰æœºåˆ¶å…è®¸æ¨¡å‹åŒæ—¶å…³æ³¨è¾“å…¥åºåˆ—çš„æ‰€æœ‰ä½ç½®ï¼Œ
            æ•æ‰åºåˆ—ä¸­çš„é•¿è·ç¦»ä¾èµ–å…³ç³»ã€‚å¤šå¤´æ³¨æ„åŠ›ï¼ˆMulti-Head Attentionï¼‰
            è¿›ä¸€æ­¥å¢å¼ºäº†æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿå­¦ä¹ å¤šç§ç±»å‹çš„å…³è”ã€‚
            
            é¢„è®­ç»ƒ-å¾®è°ƒï¼ˆPre-training + Fine-tuningï¼‰å·²æˆä¸ºNLPæ¨¡å‹çš„æ ‡å‡†èŒƒå¼ã€‚
            æ¨¡å‹é¦–å…ˆåœ¨å¤§è§„æ¨¡æ— æ ‡æ³¨æ–‡æœ¬ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå­¦ä¹ é€šç”¨è¯­è¨€è¡¨ç¤ºï¼Œ
            ç„¶ååœ¨ç‰¹å®šä»»åŠ¡çš„æ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒï¼Œè·å¾—ä»»åŠ¡ç›¸å…³çš„èƒ½åŠ›ã€‚
            """,
            metadata={"source": "Transformeræ¶æ„", "category": "æ·±åº¦å­¦ä¹ "}
        )
    ]
    
    # åˆå§‹åŒ–ç³»ç»Ÿ
    config = LLMConfig()
    qa_system = LongContextQA(config)
    
    # åŠ è½½æ–‡æ¡£
    qa_system.load_documents(sample_docs)
    
    # ç¤ºä¾‹é—®é¢˜
    questions = [
        "ä»€ä¹ˆæ˜¯å¤§è¯­è¨€æ¨¡å‹ï¼Ÿ",
        "Transformeræ¶æ„çš„æ ¸å¿ƒæ˜¯ä»€ä¹ˆï¼Ÿ",
        "æ·±åº¦å­¦ä¹ å’Œæœºå™¨å­¦ä¹ æœ‰ä»€ä¹ˆå…³ç³»ï¼Ÿ"
    ]
    
    print("\n" + "-" * 50)
    print("ğŸ” å¼€å§‹é—®ç­”æ¼”ç¤º")
    print("-" * 50)
    
    # é€ä¸ªå›ç­”é—®é¢˜
    for q in questions:
        print(f"\nâ“ é—®é¢˜: {q}")
        result = qa_system.query(q, use_llm=True, verbose=True)
        
        print(f"\nğŸ’¡ å›ç­”:")
        print(f"   {result['answer']}")
        print("-" * 50)
    
    print("\nâœ… æ¼”ç¤ºå®Œæˆ!")
    print("\nğŸ“Œ ä½¿ç”¨è¯´æ˜:")
    print("   1. ä¿®æ”¹ sample_docs åŠ è½½æ‚¨è‡ªå·±çš„æ–‡æ¡£")
    print("   2. è°ƒæ•´ LLMConfig å‚æ•°ä¼˜åŒ–æ€§èƒ½")
    print("   3. è®¾ç½® use_llm=False å¯ä»…ä½¿ç”¨æ£€ç´¢åŠŸèƒ½")


if __name__ == "__main__":
    main()
```

[2mâ±ï¸  Step 1 completed in 68.19s (total: 68.19s)[0m

[1m[96mSession Statistics:[0m
[2mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€[0m
  Session Duration: 00:01:08
  Total Messages: 3
    - User Messages: [92m1[0m
    - Assistant Replies: [94m1[0m
    - Tool Calls: [93m0[0m
  Available Tools: 8
  API Tokens Used: [95m7,587[0m
[2mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€[0m

