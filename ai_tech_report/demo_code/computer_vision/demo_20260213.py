#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è®¡ç®—æœºè§†è§‰å¤šæ¨¡æ€å›¾åƒåˆ†æç³»ç»Ÿ
Computer Vision Multimodal Image Analysis System

åŸºäºæœ€æ–°ArXivè®ºæ–‡æŠ€æœ¯çš„è§†è§‰ç†è§£ä¸ç‰¹å¾æå–æ¼”ç¤º
å®ç°äº†Transformeræ¶æ„çš„è§†è§‰æ³¨æ„åŠ›æœºåˆ¶å’Œå¤šæ¨¡æ€ç‰¹å¾èåˆ

ä¾èµ–å®‰è£…:
    pip install torch torchvision transformers opencv-python pillow numpy tqdm

ä½œè€…: AIæ•™è‚²ä¸“å®¶
åˆ›å»ºæ—¶é—´: 2026-02-13
"""

import os
import cv2
import torch
import numpy as np
from PIL import Image
from torchvision import transforms
from transformers import (
    AutoProcessor, 
    AutoModelForVision2Seq,
    CLIPVisionModel,
    CLIPProcessor
)
from typing import Dict, List, Tuple, Optional
from tqdm import tqdm
import warnings

# å¿½ç•¥è­¦å‘Šä¿¡æ¯ï¼Œä¿æŒè¾“å‡ºæ•´æ´
warnings.filterwarnings('ignore')

# è®¾ç½®è®¾å¤‡ - è‡ªåŠ¨é€‰æ‹©GPUæˆ–CPU
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"å½“å‰ä½¿ç”¨è®¾å¤‡: {device}")


class VisionFeatureExtractor:
    """
    è§†è§‰ç‰¹å¾æå–å™¨ç±»
    
    åŸºäºTransformeræ¶æ„çš„è§†è§‰ç‰¹å¾æå–ï¼Œ
    å®ç°äº†è®ºæ–‡ä¸­æåˆ°çš„æ³¨æ„åŠ›æœºåˆ¶å’Œå¤šæ¨¡æ€èåˆæŠ€æœ¯ã€‚
    
    åŠŸèƒ½:
        1. ä½¿ç”¨CLIPæ¨¡å‹æå–å›¾åƒç‰¹å¾
        2. æ”¯æŒæ‰¹é‡å›¾åƒå¤„ç†
        3. æå–å¤šå°ºåº¦ç‰¹å¾è¡¨ç¤º
    """
    
    def __init__(self, model_name: str = "openai/clip-vit-large-patch14-336"):
        """
        åˆå§‹åŒ–è§†è§‰ç‰¹å¾æå–å™¨
        
        Args:
            model_name: é¢„è®­ç»ƒæ¨¡å‹åç§°ï¼Œé»˜è®¤ä½¿ç”¨CLIPå¤§æ¨¡å‹
        """
        print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {model_name}")
        
        # åŠ è½½CLIPè§†è§‰æ¨¡å‹å’Œå¤„ç†å™¨
        self.vision_model = CLIPVisionModel.from_pretrained(model_name)
        self.processor = CLIPProcessor.from_pretrained(model_name)
        
        # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        self.vision_model.to(device)
        self.vision_model.eval()
        
        print("æ¨¡å‹åŠ è½½å®Œæˆ!")
    
    def extract_features(self, image: Image.Image) -> torch.Tensor:
        """
        ä»å•å¼ å›¾åƒä¸­æå–è§†è§‰ç‰¹å¾
        
        Args:
            image: PILæ ¼å¼çš„è¾“å…¥å›¾åƒ
            
        Returns:
            å›¾åƒç‰¹å¾å‘é‡ (torch.Tensor)
        """
        # å›¾åƒé¢„å¤„ç†
        inputs = self.processor(
            images=image, 
            return_tensors="pt"
        ).to(device)
        
        # æå–ç‰¹å¾
        with torch.no_grad():
            outputs = self.vision_model(**inputs)
            # ä½¿ç”¨[CLS]æ ‡è®°å¯¹åº”çš„ç‰¹å¾ä½œä¸ºå…¨å±€è¡¨ç¤º
            features = outputs.last_hidden_state[:, 0, :]
        
        return features
    
    def extract_multi_scale_features(self, image: Image.Image, 
                                      scales: List[float] = [1.0, 0.75, 0.5, 0.25]
                                     ) -> Dict[str, torch.Tensor]:
        """
        æå–å¤šå°ºåº¦å›¾åƒç‰¹å¾
        
        å€Ÿé‰´è®ºæ–‡ä¸­å¤šå°ºåº¦ç‰¹å¾èåˆçš„æ€æƒ³ï¼Œ
        é€šè¿‡ä¸åŒå°ºåº¦çš„å›¾åƒæå–ä¸°å¯Œçš„ç‰¹å¾è¡¨ç¤ºã€‚
        
        Args:
            image: PILæ ¼å¼çš„è¾“å…¥å›¾åƒ
            scales: ç¼©æ”¾æ¯”ä¾‹åˆ—è¡¨
            
        Returns:
            åŒ…å«ä¸åŒå°ºåº¦ç‰¹å¾çš„å­—å…¸
        """
        multi_scale_features = {}
        
        for scale in scales:
            # ç¼©æ”¾å›¾åƒ
            width, height = image.size
            new_width = int(width * scale)
            new_height = int(height * scale)
            scaled_image = image.resize((new_width, new_height))
            
            # æå–ç‰¹å¾
            features = self.extract_features(scaled_image)
            multi_scale_features[f"scale_{scale}"] = features
        
        return multi_scale_features
    
    def compare_images(self, image1: Image.Image, image2: Image.Image) -> float:
        """
        è®¡ç®—ä¸¤å¼ å›¾åƒçš„ç›¸ä¼¼åº¦
        
        åŸºäºä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—å›¾åƒç‰¹å¾ä¹‹é—´çš„è·ç¦»ã€‚
        
        Args:
            image1: ç¬¬ä¸€å¼ å›¾åƒ
            image2: ç¬¬äºŒå¼ å›¾åƒ
            
        Returns:
            ç›¸ä¼¼åº¦åˆ†æ•° (0-1ä¹‹é—´)
        """
        features1 = self.extract_features(image1)
        features2 = self.extract_features(image2)
        
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        cosine_sim = torch.nn.functional.cosine_similarity(
            features1, features2, dim=1
        )
        
        return cosine_sim.item()


class MultimodalImageAnalyzer:
    """
    å¤šæ¨¡æ€å›¾åƒåˆ†æå™¨
    
    ç»“åˆè§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯çš„æ™ºèƒ½å›¾åƒåˆ†æç³»ç»Ÿï¼Œ
    å®ç°äº†è®ºæ–‡ä¸­çš„å¤šæ¨¡æ€æ€ç»´é“¾æ¨ç†ã€‚
    
    åŠŸèƒ½:
        1. å›¾åƒæè¿°ç”Ÿæˆ
        2. è§†è§‰é—®ç­”
        3. å›¾åƒç›¸ä¼¼åº¦æ¯”è¾ƒ
    """
    
    def __init__(self, model_name: str = "Salesforce/instructblip-flan-t5-large"):
        """
        åˆå§‹åŒ–å¤šæ¨¡æ€åˆ†æå™¨
        
        Args:
            model_name: å¤šæ¨¡æ€æ¨¡å‹åç§°
        """
        print(f"æ­£åœ¨åŠ è½½å¤šæ¨¡æ€æ¨¡å‹: {model_name}")
        
        # åŠ è½½æŒ‡ä»¤è°ƒæ•´çš„å›¾åƒåˆ°æ–‡æœ¬æ¨¡å‹
        self.model = AutoModelForVision2Seq.from_pretrained(model_name)
        self.processor = AutoProcessor.from_pretrained(model_name)
        
        self.model.to(device)
        self.model.eval()
        
        print("å¤šæ¨¡æ€æ¨¡å‹åŠ è½½å®Œæˆ!")
    
    def generate_description(self, image: Image.Image, 
                           prompt: str = "Describe this image in detail."
                           ) -> str:
        """
        ç”Ÿæˆå›¾åƒæè¿°
        
        Args:
            image: è¾“å…¥å›¾åƒ
            prompt: æç¤ºè¯
            
        Returns:
            ç”Ÿæˆçš„æè¿°æ–‡æœ¬
        """
        inputs = self.processor(
            images=image,
            text=prompt,
            return_tensors="pt"
        ).to(device)
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=150,
                num_beams=5,
                temperature=0.7,
                do_sample=True
            )
        
        description = self.processor.decode(outputs[0], skip_special_tokens=True)
        return description
    
    def answer_question(self, image: Image.Image, 
                       question: str) -> str:
        """
        å›ç­”å…³äºå›¾åƒçš„é—®é¢˜
        
        Args:
            image: è¾“å…¥å›¾åƒ
            question: é—®é¢˜
            
        Returns:
            å›ç­”æ–‡æœ¬
        """
        prompt = f"Question: {question} Answer:"
        return self.generate_description(image, prompt)


class ImageProcessingUtils:
    """
    å›¾åƒå¤„ç†å·¥å…·ç±»
    
    æä¾›åŸºç¡€çš„å›¾åƒé¢„å¤„ç†å’Œåå¤„ç†åŠŸèƒ½ã€‚
    
    åŠŸèƒ½:
        1. å›¾åƒåŠ è½½å’Œä¿å­˜
        2. å›¾åƒå¢å¼º
        3. ç‰¹å¾å¯è§†åŒ–
    """
    
    @staticmethod
    def load_image(image_path: str) -> Optional[Image.Image]:
        """
        åŠ è½½å›¾åƒæ–‡ä»¶
        
        Args:
            image_path: å›¾åƒæ–‡ä»¶è·¯å¾„
            
        Returns:
            PILå›¾åƒå¯¹è±¡ï¼Œå¤±è´¥è¿”å›None
        """
        try:
            # ä½¿ç”¨OpenCVåŠ è½½å›¾åƒ
            img_cv2 = cv2.imread(image_path)
            if img_cv2 is None:
                print(f"æ— æ³•åŠ è½½å›¾åƒ: {image_path}")
                return None
            
            # è½¬æ¢ä¸ºRGBæ ¼å¼ (OpenCVé»˜è®¤BGR)
            img_rgb = cv2.cvtColor(img_cv2, cv2.COLOR_BGR2RGB)
            
            # è½¬æ¢ä¸ºPILæ ¼å¼
            return Image.fromarray(img_rgb)
        
        except Exception as e:
            print(f"åŠ è½½å›¾åƒæ—¶å‡ºé”™: {e}")
            return None
    
    @staticmethod
    def save_features_to_file(features: torch.Tensor, 
                             output_path: str,
                             format: str = "pt") -> None:
        """
        ä¿å­˜ç‰¹å¾å‘é‡åˆ°æ–‡ä»¶
        
        Args:
            features: ç‰¹å¾å‘é‡
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            format: ä¿å­˜æ ¼å¼ ("pt"ä¸ºPyTorchæ ¼å¼ï¼Œ"npy"ä¸ºNumPyæ ¼å¼)
        """
        if format == "pt":
            torch.save(features, output_path)
        elif format == "npy":
            np.save(output_path, features.cpu().numpy())
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ ¼å¼: {format}")
        
        print(f"ç‰¹å¾å·²ä¿å­˜åˆ°: {output_path}")
    
    @staticmethod
    def visualize_attention(image: Image.Image, 
                           attention_weights: np.ndarray,
                           output_path: str,
                           alpha: float = 0.5) -> None:
        """
        å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡
        
        å°†æ³¨æ„åŠ›æƒé‡å åŠ åˆ°åŸå§‹å›¾åƒä¸Šã€‚
        
        Args:
            image: åŸå§‹å›¾åƒ
            attention_weights: æ³¨æ„åŠ›æƒé‡çŸ©é˜µ
            output_path: è¾“å‡ºå›¾åƒè·¯å¾„
            alpha: å åŠ é€æ˜åº¦
        """
        # è°ƒæ•´æ³¨æ„åŠ›æƒé‡å¤§å°ä»¥åŒ¹é…å›¾åƒ
        attention_map = cv2.resize(attention_weights, image.size)
        
        # åº”ç”¨é¢œè‰²æ˜ å°„
        heatmap = cv2.applyColorMap(
            np.uint8(255 * attention_map), 
            cv2.COLORMAP_JET
        )
        
        # è½¬æ¢ä¸ºRGB
        heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)
        
        # å åŠ åˆ°åŸå›¾
        original_array = np.array(image)
        result = cv2.addWeighted(
            np.array(original_array), 
            1 - alpha, 
            heatmap, 
            alpha, 
            0
        )
        
        # ä¿å­˜ç»“æœ
        cv2.imwrite(output_path, cv2.cvtColor(result, cv2.COLOR_RGB2BGR))
        print(f"æ³¨æ„åŠ›å¯è§†åŒ–å·²ä¿å­˜åˆ°: {output_path}")


def demo_image_analysis():
    """
    æ¼”ç¤ºå‡½æ•°ï¼šå±•ç¤ºè®¡ç®—æœºè§†è§‰ç³»ç»Ÿçš„å®Œæ•´åŠŸèƒ½
    
    è¿™ä¸ªå‡½æ•°æ¼”ç¤ºäº†å¦‚ä½•ä½¿ç”¨ä¸Šè¿°ç±»è¿›è¡Œå›¾åƒåˆ†æï¼Œ
    åŒ…æ‹¬ç‰¹å¾æå–ã€ç›¸ä¼¼åº¦æ¯”è¾ƒå’Œå¤šæ¨¡æ€æ¨ç†ã€‚
    """
    print("\n" + "="*60)
    print("è®¡ç®—æœºè§†è§‰å¤šæ¨¡æ€å›¾åƒåˆ†æç³»ç»Ÿæ¼”ç¤º")
    print("="*60 + "\n")
    
    # åˆ›å»ºç¤ºä¾‹å›¾åƒï¼ˆå¦‚æœæ²¡æœ‰çœŸå®å›¾åƒï¼‰
    print("åˆ›å»ºç¤ºä¾‹æµ‹è¯•å›¾åƒ...")
    
    # åˆ›å»ºæµ‹è¯•å›¾åƒ1ï¼šçº¢è‰²èƒŒæ™¯çš„çŸ©å½¢
    img1 = Image.new('RGB', (224, 224), color=(255, 100, 100))
    img1.save("test_image_1.jpg")
    
    # åˆ›å»ºæµ‹è¯•å›¾åƒ2ï¼šè“è‰²èƒŒæ™¯çš„çŸ©å½¢ï¼ˆä¸å›¾åƒ1ç›¸ä¼¼ï¼‰
    img2 = Image.new('RGB', (224, 224), color=(100, 100, 255))
    img2.save("test_image_2.jpg")
    
    # åˆ›å»ºæµ‹è¯•å›¾åƒ3ï¼šç»¿è‰²èƒŒæ™¯çš„çŸ©å½¢ï¼ˆä¸å›¾åƒ1ä¸å¤ªç›¸ä¼¼ï¼‰
    img3 = Image.new('RGB', (224, 224), color=(100, 255, 100))
    img3.save("test_image_3.jpg")
    
    print("æµ‹è¯•å›¾åƒåˆ›å»ºå®Œæˆï¼\n")
    
    # åˆå§‹åŒ–ç‰¹å¾æå–å™¨
    print("1. åˆå§‹åŒ–è§†è§‰ç‰¹å¾æå–å™¨...")
    extractor = VisionFeatureExtractor()
    
    # åŠ è½½æµ‹è¯•å›¾åƒ
    test_images = [
        Image.open("test_image_1.jpg"),
        Image.open("test_image_2.jpg"),
        Image.open("test_image_3.jpg")
    ]
    
    print("\n2. æå–å›¾åƒç‰¹å¾...")
    for i, img in enumerate(test_images):
        # æå–å•å°ºåº¦ç‰¹å¾
        features = extractor.extract_features(img)
        print(f"   å›¾åƒ{i+1}ç‰¹å¾ç»´åº¦: {features.shape}")
        
        # æå–å¤šå°ºåº¦ç‰¹å¾
        multi_scale = extractor.extract_multi_scale_features(img)
        print(f"   å›¾åƒ{i+1}å¤šå°ºåº¦ç‰¹å¾æ•°é‡: {len(multi_scale)}")
    
    print("\n3. è®¡ç®—å›¾åƒç›¸ä¼¼åº¦...")
    # è®¡ç®—å›¾åƒ1å’Œå›¾åƒ2çš„ç›¸ä¼¼åº¦
    similarity_12 = extractor.compare_images(test_images[0], test_images[1])
    print(f"   å›¾åƒ1ä¸å›¾åƒ2ç›¸ä¼¼åº¦: {similarity_12:.4f}")
    
    # è®¡ç®—å›¾åƒ1å’Œå›¾åƒ3çš„ç›¸ä¼¼åº¦
    similarity_13 = extractor.compare_images(test_images[0], test_images[2])
    print(f"   å›¾åƒ1ä¸å›¾åƒ3ç›¸ä¼¼åº¦: {similarity_13:.4f}")
    
    # è®¡ç®—å›¾åƒ2å’Œå›¾åƒ3çš„ç›¸ä¼¼åº¦
    similarity_23 = extractor.compare_images(test_images[1], test_images[2])
    print(f"   å›¾åƒ2ä¸å›¾åƒ3ç›¸ä¼¼åº¦: {similarity_23:.4f}")
    
    print("\n4. æµ‹è¯•å¤šæ¨¡æ€åˆ†æåŠŸèƒ½...")
    analyzer = MultimodalImageAnalyzer()
    
    # ç”Ÿæˆå›¾åƒæè¿°
    description = analyzer.generate_description(
        test_images[0], 
        "Describe this image in detail."
    )
    print(f"   å›¾åƒæè¿°: {description}")
    
    # å›ç­”é—®é¢˜
    answer = analyzer.answer_question(
        test_images[0], 
        "What colors are present in this image?"
    )
    print(f"   é¢œè‰²é—®ç­”: {answer}")
    
    print("\n5. ä¿å­˜ç‰¹å¾å‘é‡...")
    # ä¿å­˜å›¾åƒ1çš„ç‰¹å¾
    features = extractor.extract_features(test_images[0])
    ImageProcessingUtils.save_features_to_file(
        features, 
        "image_features.pt"
    )
    
    # æ¸…ç†ä¸´æ—¶æµ‹è¯•å›¾åƒ
    print("\n6. æ¸…ç†æµ‹è¯•æ–‡ä»¶...")
    for i in range(1, 4):
        filename = f"test_image_{i}.jpg"
        if os.path.exists(filename):
            os.remove(filename)
            print(f"   å·²åˆ é™¤: {filename}")
    
    print("\n" + "="*60)
    print("æ¼”ç¤ºå®Œæˆï¼")
    print("="*60)
    print("\nåŠŸèƒ½æ€»ç»“:")
    print("  âœ“ è§†è§‰ç‰¹å¾æå– (åŸºäºTransformeræ¶æ„)")
    print("  âœ“ å¤šå°ºåº¦ç‰¹å¾åˆ†æ")
    print("  âœ“ å›¾åƒç›¸ä¼¼åº¦æ¯”è¾ƒ")
    print("  âœ“ å¤šæ¨¡æ€æè¿°ç”Ÿæˆ")
    print("  âœ“ è§†è§‰é—®ç­”åŠŸèƒ½")
    print("\næ³¨æ„äº‹é¡¹:")
    print("  - é¦–æ¬¡è¿è¡Œä¼šè‡ªåŠ¨ä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹")
    print("  - å»ºè®®åœ¨GPUç¯å¢ƒä¸‹è¿è¡Œä»¥è·å¾—æ›´å¥½æ€§èƒ½")
    print("  - å¯æ›¿æ¢æµ‹è¯•å›¾åƒè·¯å¾„è¿›è¡Œå®é™…åº”ç”¨")


def main():
    """
    ä¸»å‡½æ•°
    
    ç¨‹åºå…¥å£ç‚¹ï¼Œè´Ÿè´£åè°ƒå„ä¸ªæ¨¡å—çš„æ‰§è¡Œã€‚
    """
    try:
        # è¿è¡Œæ¼”ç¤º
        demo_image_analysis()
    
    except KeyboardInterrupt:
        print("\n\nç”¨æˆ·ä¸­æ–­ç¨‹åºæ‰§è¡Œ")
    
    except Exception as e:
        print(f"\nç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
```

[2mâ±ï¸  Step 1 completed in 71.46s (total: 71.46s)[0m

[2mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€[0m


[1m[96mSession Statistics:[0m
[2mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€[0m
  Session Duration: 00:01:11
  Total Messages: 3
    - User Messages: [92m1[0m
    - Assistant Replies: [94m1[0m
    - Tool Calls: [93m0[0m
  Available Tools: 8
  API Tokens Used: [95m7,150[0m
[2mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€[0m

[96mCleaning up MCP connections...[0m
[32mâœ… Cleanup complete[0m

